/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:35:11,767[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:35:11,767[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_03[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:35:15,047[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:35:15,048[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       net:                                                                    
│         _target_: src.models.components.gauss_net.SimpleDenseNet_ELU_gauss    
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss                                                      
│         name: Dense_3_Hidden_128_64_32_ELU                                    
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - Dense_3_Hidden_128_64_32_ELU                                        
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── Dense_3_Hidden_128_64_32_ELU                                            
[[36m2022-06-15 09:35:15,087[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:35:15,103[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:35:15,508[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpamza028f[0m
[[36m2022-06-15 09:35:15,509[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpamza028f/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:35:15,517[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:35:15,518[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:35:15,518[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:35:15,519[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:35:15,519[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_093516-6g9js550
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_ELU
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss/runs/6g9js550
[[36m2022-06-15 09:35:21,003[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:35:21,041[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:35:21,045[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:35:23,327[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type                     ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ SimpleDenseNet_ELU_gauss │ 23.8 K │
│ 1  │ net.model    │ Sequential               │ 23.8 K │
│ 2  │ net.model.0  │ Linear                   │ 12.9 K │
│ 3  │ net.model.1  │ BatchNorm1d              │    256 │
│ 4  │ net.model.2  │ ELU                      │      0 │
│ 5  │ net.model.3  │ Linear                   │  8.3 K │
│ 6  │ net.model.4  │ BatchNorm1d              │    128 │
│ 7  │ net.model.5  │ ELU                      │      0 │
│ 8  │ net.model.6  │ Linear                   │  2.1 K │
│ 9  │ net.model.7  │ BatchNorm1d              │     64 │
│ 10 │ net.model.8  │ ELU                      │      0 │
│ 11 │ net.model.9  │ Linear                   │     66 │
│ 12 │ criterion    │ MSELoss                  │      0 │
│ 13 │ train_acc    │ MeanSquaredError         │      0 │
│ 14 │ val_acc      │ MeanSquaredError         │      0 │
│ 15 │ test_acc     │ MeanSquaredError         │      0 │
│ 16 │ val_acc_best │ MinMetric                │      0 │
└────┴──────────────┴──────────────────────────┴────────┘
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        284.09it/s loss: 0.905      
                                   0:00:00                     v_num: s550      
                                                               val/acc: 0.479   
                                                               val/acc_best:    
                                                               0.384 train/acc: 
                                                               0.858            
[[36m2022-06-15 09:39:34,654[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:39:34,656[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_ELU/2022-06-15_09-35-11/experiment=gauss_03/checkpoints/epoch_002.ckpt[0m
[[36m2022-06-15 09:39:34,660[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:39:34,660[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_ELU/2022-06-15_09-35-11/experiment=gauss_03/checkpoints/epoch_002.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.7315894365310669     │
│         test/loss         │    0.7315894365310669     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 783.65it/s 
[[36m2022-06-15 09:39:35,053[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.299 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc ▇█▆▄▇▄▂▃▄▃▃▂▃▄▃▂▂▂▂▄▁▂▁▂▁▃▂▁▁▂▁▂▂▂▂▄▁▂▂▃
wandb:        val/acc_best █▇▆▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss ▇█▆▄▇▄▂▃▄▃▃▂▃▄▃▂▂▂▂▄▁▂▁▂▁▃▂▁▁▂▁▂▂▂▂▄▁▂▂▃
wandb: 
wandb: Run summary:
wandb:               epoch 2
wandb:            test/acc 0.73159
wandb:           test/loss 0.73159
wandb:           train/acc 0.85764
wandb:          train/loss 0.85764
wandb: trainer/global_step 62500
wandb:             val/acc 0.47857
wandb:        val/acc_best 0.38381
wandb:            val/loss 0.47857
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_ELU: https://wandb.ai/rshimogawa/nngauss/runs/6g9js550
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_093516-6g9js550/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4f2dfd63a0>
True
[[36m2022-06-15 09:39:40,017[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_ELU/2022-06-15_09-35-11/experiment=gauss_03/checkpoints/epoch_002.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:39:41,904[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:39:41,904[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_04[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:39:43,168[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:39:43,169[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       net:                                                                    
│         _target_: src.models.components.gauss_net.SimpleDenseNet_SS_gauss     
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss                                                      
│         name: Dense_3_Hidden_128_64_32_SoftShrink                             
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - Dense_3_Hidden_128_64_32_SoftShrink                                 
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── Dense_3_Hidden_128_64_32_SoftShrink                                     
[[36m2022-06-15 09:39:43,197[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:39:43,199[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:39:43,402[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpmlro1m_n[0m
[[36m2022-06-15 09:39:43,402[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpmlro1m_n/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:39:43,409[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:39:43,410[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:39:43,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:39:43,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:39:43,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_093944-36m2alcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_SoftShrink
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss/runs/36m2alcn
[[36m2022-06-15 09:39:48,231[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:39:48,259[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:39:48,262[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:39:50,358[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type                    ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ SimpleDenseNet_SS_gauss │ 23.8 K │
│ 1  │ net.model    │ Sequential              │ 23.8 K │
│ 2  │ net.model.0  │ Linear                  │ 12.9 K │
│ 3  │ net.model.1  │ BatchNorm1d             │    256 │
│ 4  │ net.model.2  │ Softshrink              │      0 │
│ 5  │ net.model.3  │ Linear                  │  8.3 K │
│ 6  │ net.model.4  │ BatchNorm1d             │    128 │
│ 7  │ net.model.5  │ Softshrink              │      0 │
│ 8  │ net.model.6  │ Linear                  │  2.1 K │
│ 9  │ net.model.7  │ BatchNorm1d             │     64 │
│ 10 │ net.model.8  │ Softshrink              │      0 │
│ 11 │ net.model.9  │ Linear                  │     66 │
│ 12 │ criterion    │ MSELoss                 │      0 │
│ 13 │ train_acc    │ MeanSquaredError        │      0 │
│ 14 │ val_acc      │ MeanSquaredError        │      0 │
│ 15 │ test_acc     │ MeanSquaredError        │      0 │
│ 16 │ val_acc_best │ MinMetric               │      0 │
└────┴──────────────┴─────────────────────────┴────────┘
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        281.72it/s loss: 0.217      
                                   0:00:00                     v_num: alcn      
                                                               val/acc: 0.147   
                                                               val/acc_best:    
                                                               0.122 train/acc: 
                                                               0.242            
[[36m2022-06-15 09:44:01,630[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:44:01,632[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_SoftShrink/2022-06-15_09-39-41/experiment=gauss_04/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-15 09:44:01,636[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:44:01,637[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_SoftShrink/2022-06-15_09-39-41/experiment=gauss_04/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    1.9462816715240479     │
│         test/loss         │    1.9462816715240479     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 778.22it/s 
[[36m2022-06-15 09:44:02,112[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.362 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.362 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 1.94628
wandb:           test/loss 1.94628
wandb:           train/acc 0.24216
wandb:          train/loss 0.24216
wandb: trainer/global_step 62500
wandb:             val/acc 0.14683
wandb:        val/acc_best 0.12181
wandb:            val/loss 0.14683
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_SoftShrink: https://wandb.ai/rshimogawa/nngauss/runs/36m2alcn
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_093944-36m2alcn/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f642504c310>
True
[[36m2022-06-15 09:44:06,358[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_SoftShrink/2022-06-15_09-39-41/experiment=gauss_04/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:44:08,386[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:44:08,387[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_05[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:44:09,705[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:44:09,705[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       net:                                                                    
│         _target_: src.models.components.gauss_net.SimpleDenseNet_LR_gauss     
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss                                                      
│         name: Dense_3_Hidden_128_64_32_LeakyReLU                              
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - Dense_3_Hidden_128_64_32_LeakyReLU                                  
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── Dense_3_Hidden_128_64_32_LeakyReLU                                      
[[36m2022-06-15 09:44:09,733[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:44:09,735[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:44:09,932[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpx528yzns[0m
[[36m2022-06-15 09:44:09,933[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpx528yzns/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:44:09,940[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:44:09,941[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:44:09,941[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:44:09,942[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:44:09,942[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_094410-16sdi56t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_LeakyReLU
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss/runs/16sdi56t
[[36m2022-06-15 09:44:14,636[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:44:14,666[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:44:14,667[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:44:14,670[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:44:16,757[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type                    ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ SimpleDenseNet_LR_gauss │ 23.8 K │
│ 1  │ net.model    │ Sequential              │ 23.8 K │
│ 2  │ net.model.0  │ Linear                  │ 12.9 K │
│ 3  │ net.model.1  │ BatchNorm1d             │    256 │
│ 4  │ net.model.2  │ LeakyReLU               │      0 │
│ 5  │ net.model.3  │ Linear                  │  8.3 K │
│ 6  │ net.model.4  │ BatchNorm1d             │    128 │
│ 7  │ net.model.5  │ LeakyReLU               │      0 │
│ 8  │ net.model.6  │ Linear                  │  2.1 K │
│ 9  │ net.model.7  │ BatchNorm1d             │     64 │
│ 10 │ net.model.8  │ LeakyReLU               │      0 │
│ 11 │ net.model.9  │ Linear                  │     66 │
│ 12 │ criterion    │ MSELoss                 │      0 │
│ 13 │ train_acc    │ MeanSquaredError        │      0 │
│ 14 │ val_acc      │ MeanSquaredError        │      0 │
│ 15 │ test_acc     │ MeanSquaredError        │      0 │
│ 16 │ val_acc_best │ MinMetric               │      0 │
└────┴──────────────┴─────────────────────────┴────────┘
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        281.87it/s loss: 0.25 v_num:
                                   0:00:00                     i56t val/acc:    
                                                               0.124            
                                                               val/acc_best:    
                                                               0.112 train/acc: 
                                                               0.257            
[[36m2022-06-15 09:48:29,372[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:48:29,374[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_LeakyReLU/2022-06-15_09-44-08/experiment=gauss_05/checkpoints/epoch_001.ckpt[0m
[[36m2022-06-15 09:48:29,378[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:48:29,379[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_LeakyReLU/2022-06-15_09-44-08/experiment=gauss_05/checkpoints/epoch_001.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.2891356945037842     │
│         test/loss         │    0.2891356945037842     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 760.21it/s 
[[36m2022-06-15 09:48:29,777[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▆▄▅▅▄▃▂▃▃▂▄▁▃▃▅▃▄▃▂▂▄▂▃▃▃▁▄▃▂▂▄▅▂▄▃▄▂▄▂
wandb:        val/acc_best █▆▄▄▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▆▄▅▅▄▃▂▃▃▂▄▁▃▃▅▃▄▃▂▂▄▂▃▃▃▁▄▃▂▂▄▅▂▄▃▄▂▄▂
wandb: 
wandb: Run summary:
wandb:               epoch 1
wandb:            test/acc 0.28914
wandb:           test/loss 0.28914
wandb:           train/acc 0.25665
wandb:          train/loss 0.25665
wandb: trainer/global_step 62500
wandb:             val/acc 0.12449
wandb:        val/acc_best 0.11161
wandb:            val/loss 0.12449
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_LeakyReLU: https://wandb.ai/rshimogawa/nngauss/runs/16sdi56t
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_094410-16sdi56t/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f064be0ae80>
True
[[36m2022-06-15 09:48:34,041[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_LeakyReLU/2022-06-15_09-44-08/experiment=gauss_05/checkpoints/epoch_001.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:48:35,988[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:48:35,988[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_06[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:48:37,284[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:48:37,285[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       net:                                                                    
│         _target_: src.models.components.gauss_net.SimpleDenseNet_LL_gauss     
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss                                                      
│         name: Dense_3_Hidden_128_64_32_Linear                                 
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - Dense_3_Hidden_128_64_32_Linear                                     
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── Dense_3_Hidden_128_64_32_Linear                                         
[[36m2022-06-15 09:48:37,313[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:48:37,316[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:48:37,494[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpwuxkl9x8[0m
[[36m2022-06-15 09:48:37,494[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpwuxkl9x8/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:48:37,500[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:48:37,501[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:48:37,502[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:48:37,502[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:48:37,503[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_094838-3i5ed9q4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_Linear
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss/runs/3i5ed9q4
[[36m2022-06-15 09:48:42,394[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:48:42,422[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:48:42,423[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:48:42,426[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:48:44,525[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type                    ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ SimpleDenseNet_LL_gauss │ 23.8 K │
│ 1  │ net.model    │ Sequential              │ 23.8 K │
│ 2  │ net.model.0  │ Linear                  │ 12.9 K │
│ 3  │ net.model.1  │ BatchNorm1d             │    256 │
│ 4  │ net.model.2  │ Linear                  │  8.3 K │
│ 5  │ net.model.3  │ BatchNorm1d             │    128 │
│ 6  │ net.model.4  │ Linear                  │  2.1 K │
│ 7  │ net.model.5  │ BatchNorm1d             │     64 │
│ 8  │ net.model.6  │ Linear                  │     66 │
│ 9  │ criterion    │ MSELoss                 │      0 │
│ 10 │ train_acc    │ MeanSquaredError        │      0 │
│ 11 │ val_acc      │ MeanSquaredError        │      0 │
│ 12 │ test_acc     │ MeanSquaredError        │      0 │
│ 13 │ val_acc_best │ MinMetric               │      0 │
└────┴──────────────┴─────────────────────────┴────────┘
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        287.33it/s loss: 4.41 v_num:
                                   0:00:00                     d9q4 val/acc:    
                                                               4.251            
                                                               val/acc_best:    
                                                               4.201 train/acc: 
                                                               4.514            
[[36m2022-06-15 09:52:52,827[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:52:52,829[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_Linear/2022-06-15_09-48-35/experiment=gauss_06/checkpoints/epoch_077.ckpt[0m
[[36m2022-06-15 09:52:52,833[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:52:52,833[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_Linear/2022-06-15_09-48-35/experiment=gauss_06/checkpoints/epoch_077.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    4.5567851066589355     │
│         test/loss         │    4.5567851066589355     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 769.75it/s 
[[36m2022-06-15 09:52:53,279[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc ▃▂▂▁▂▃▁▅▄▁▂▃▂▇▃▁▁▂▂▁▂▃▁▂▄▃▂▃▂▂▄▂▃▂▅█▂▁▅▃
wandb:        val/acc_best █▅▅▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss ▃▂▂▁▂▃▁▅▄▁▂▃▂▇▃▁▁▂▂▁▂▃▁▂▄▃▂▃▂▂▄▂▃▂▅█▂▁▅▃
wandb: 
wandb: Run summary:
wandb:               epoch 77
wandb:            test/acc 4.55679
wandb:           test/loss 4.55679
wandb:           train/acc 4.51439
wandb:          train/loss 4.51439
wandb: trainer/global_step 62500
wandb:             val/acc 4.25132
wandb:        val/acc_best 4.20076
wandb:            val/loss 4.25132
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_Linear: https://wandb.ai/rshimogawa/nngauss/runs/3i5ed9q4
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_094838-3i5ed9q4/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4fead456a0>
True
[[36m2022-06-15 09:52:57,918[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_Linear/2022-06-15_09-48-35/experiment=gauss_06/checkpoints/epoch_077.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:52:59,898[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:52:59,898[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_07[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:53:01,366[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:53:01,366[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       net:                                                                    
│         _target_: src.models.components.gauss_net.SimpleDenseNet_CNN_gauss    
│         input_size: 100                                                       
│         lin1_size: 64                                                         
│         lin2_size: 32                                                         
│         lin3_size: 16                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss                                                      
│         name: CNN                                                             
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - CNN                                                                 
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── CNN                                                                     
[[36m2022-06-15 09:53:01,395[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:53:01,397[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
Error executing job with overrides: ['experiment=gauss_07']
Error locating target 'src.models.components.gauss_net.SimpleDenseNet_CNN_gauss', see chained exception above.
full_key: model.net

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 10:29:00,296[0m][[35mHYDRA[0m] Launching 0 jobs locally[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 11:37:16,705[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 11:37:16,705[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_11_LR_MAE[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 11:37:17,789[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 11:37:17,790[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module_mae.gaussLitModule                    
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       net:                                                                    
│         _target_: src.models.components.gauss_net.CNN2_gauss                  
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss                                                      
│         name: Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE                    
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE                        
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE                            
[[36m2022-06-15 11:37:17,819[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 11:37:17,822[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module_mae.gaussLitModule>[0m
[[36m2022-06-15 11:37:18,048[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp0rfgyj6v[0m
[[36m2022-06-15 11:37:18,048[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp0rfgyj6v/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 11:37:18,055[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 11:37:18,056[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 11:37:18,057[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 11:37:18,057[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 11:37:18,058[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_113718-2yrhbtdg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss/runs/2yrhbtdg
[[36m2022-06-15 11:37:22,792[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 11:37:22,819[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 11:37:22,822[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 11:37:24,901[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ CNN2_gauss        │ 26.0 K │
│ 1  │ net.model    │ Sequential        │ 26.0 K │
│ 2  │ net.model.0  │ Unflatten         │      0 │
│ 3  │ net.model.1  │ Conv1d            │      8 │
│ 4  │ net.model.2  │ Flatten           │      0 │
│ 5  │ net.model.3  │ Linear            │ 12.9 K │
│ 6  │ net.model.4  │ BatchNorm1d       │    256 │
│ 7  │ net.model.5  │ LeakyReLU         │      0 │
│ 8  │ net.model.6  │ Linear            │  8.3 K │
│ 9  │ net.model.7  │ BatchNorm1d       │    128 │
│ 10 │ net.model.8  │ LeakyReLU         │      0 │
│ 11 │ net.model.9  │ Linear            │  2.1 K │
│ 12 │ net.model.10 │ BatchNorm1d       │     64 │
│ 13 │ net.model.11 │ LeakyReLU         │      0 │
│ 14 │ net.model.12 │ Linear            │  1.1 K │
│ 15 │ net.model.13 │ BatchNorm1d       │     64 │
│ 16 │ net.model.14 │ LeakyReLU         │      0 │
│ 17 │ net.model.15 │ Linear            │  1.1 K │
│ 18 │ net.model.16 │ BatchNorm1d       │     64 │
│ 19 │ net.model.17 │ LeakyReLU         │      0 │
│ 20 │ net.model.18 │ Linear            │     66 │
│ 21 │ criterion    │ L1Loss            │      0 │
│ 22 │ train_acc    │ MeanAbsoluteError │      0 │
│ 23 │ val_acc      │ MeanAbsoluteError │      0 │
│ 24 │ test_acc     │ MeanAbsoluteError │      0 │
│ 25 │ val_acc_best │ MinMetric         │      0 │
└────┴──────────────┴───────────────────┴────────┘
Trainable params: 26.0 K                                                        
Non-trainable params: 0                                                         
Total params: 26.0 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 149 ━━━━━━━━━━━━━━━━ 704/704 0:00:03 •        216.90it/s loss: 0.303      
                                   0:00:00                     v_num: btdg      
                                                               val/acc: 0.181   
                                                               val/acc_best:    
                                                               0.151 train/acc: 
                                                               0.274            
[[36m2022-06-15 11:45:33,787[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 11:45:33,790[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE/2022-06-15_11-37-16/experiment=gauss_11_LR_MAE/checkpoints/epoch_049.ckpt[0m
[[36m2022-06-15 11:45:33,796[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 11:45:33,796[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE/2022-06-15_11-37-16/experiment=gauss_11_LR_MAE/checkpoints/epoch_049.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │      0.3792724609375      │
│         test/loss         │      0.3792724609375      │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 674.28it/s 
[[36m2022-06-15 11:45:34,206[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.362 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▆▅▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▆▅▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:             val/acc ▅▅▄▅▄▂▂▅▃▇▃▄▃█▅▃▅▂▂▃▂▄▄▄▃▄▅▁▂▆▂▄▅▃▇▁▃▆▃▂
wandb:        val/acc_best ██▅▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss ▅▅▄▅▄▂▂▅▃▇▃▄▃█▅▃▅▂▂▃▂▄▄▄▃▄▅▁▂▆▂▄▅▃▇▁▃▆▃▂
wandb: 
wandb: Run summary:
wandb:               epoch 49
wandb:            test/acc 0.37927
wandb:           test/loss 0.37927
wandb:           train/acc 0.27363
wandb:          train/loss 0.27363
wandb: trainer/global_step 93750
wandb:             val/acc 0.18145
wandb:        val/acc_best 0.15089
wandb:            val/loss 0.18145
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE: https://wandb.ai/rshimogawa/nngauss/runs/2yrhbtdg
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_113718-2yrhbtdg/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f45a1847550>
True
[[36m2022-06-15 11:45:38,300[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE/2022-06-15_11-37-16/experiment=gauss_11_LR_MAE/checkpoints/epoch_049.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:23:51,496[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:23:51,496[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:23:52,510[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:23:52,510[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H3_ReLU_128_64_32                                                 
│       net:                                                                    
│         _target_: src.models.components.NN_net.H3_ReLU                        
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H3_ReLU_128_64_32                                               
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H3_ReLU_128_64_32                                                   
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H3_ReLU_128_64_32                                                       
[[36m2022-06-16 12:23:52,539[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:23:52,542[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:23:52,736[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmplo09_ea2[0m
[[36m2022-06-16 12:23:52,736[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmplo09_ea2/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:23:52,742[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:23:52,743[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:23:52,744[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:23:52,744[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:23:52,744[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_122353-6psq7it0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_ReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/6psq7it0
[[36m2022-06-16 12:23:58,545[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:23:58,577[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:23:58,577[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:23:58,583[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:24:01,228[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H3_ReLU          │ 23.3 K │
│ 1  │ net.model    │ Sequential       │ 23.3 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ReLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ReLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ReLU             │      0 │
│ 8  │ net.model.6  │ Linear           │     66 │
│ 9  │ criterion    │ MSELoss          │      0 │
│ 10 │ train_acc    │ MeanSquaredError │      0 │
│ 11 │ val_acc      │ MeanSquaredError │      0 │
│ 12 │ test_acc     │ MeanSquaredError │      0 │
│ 13 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        334.18it/s loss: 0.0187     
                                   0:00:00                     v_num: 7it0      
                                                               val/acc: 0.019   
                                                               val/acc_best:    
                                                               0.019 train/acc: 
                                                               0.021            
[[36m2022-06-16 12:27:29,852[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:27:30,456[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ReLU_128_64_32/2022-06-16_12-23-51/experiment=NN/H3_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:27:30,459[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:27:30,460[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ReLU_128_64_32/2022-06-16_12-23-51/experiment=NN/H3_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.21474191546440125    │
│         test/loss         │    0.21474191546440125    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 842.50it/s 
[[36m2022-06-16 12:27:30,925[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.393 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.398 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.21474
wandb:           test/loss 0.21474
wandb:           train/acc 0.02059
wandb:          train/loss 0.02059
wandb: trainer/global_step 62500
wandb:             val/acc 0.0189
wandb:        val/acc_best 0.0189
wandb:            val/loss 0.0189
wandb: 
wandb: Synced H3_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/6psq7it0
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_122353-6psq7it0/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fb533154250>
True
[[36m2022-06-16 12:27:35,640[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ReLU_128_64_32/2022-06-16_12-23-51/experiment=NN/H3_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:27:37,625[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:27:37,625[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:27:38,676[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:27:38,677[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H4_ReLU_128_64_32                                                 
│       net:                                                                    
│         _target_: src.models.components.NN_net.H4_ReLU                        
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H4_ReLU_128_64_32                                               
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H4_ReLU_128_64_32                                                   
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H4_ReLU_128_64_32                                                       
[[36m2022-06-16 12:27:38,706[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:27:38,709[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:27:38,903[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp78u34q5k[0m
[[36m2022-06-16 12:27:38,904[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp78u34q5k/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:27:38,910[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:27:38,911[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:27:38,912[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:27:38,912[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:27:38,913[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_122739-1qvh86ai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_ReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1qvh86ai
[[36m2022-06-16 12:27:43,800[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:27:43,830[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:27:43,831[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:27:43,835[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:27:46,459[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H4_ReLU          │ 24.4 K │
│ 1  │ net.model    │ Sequential       │ 24.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ReLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ReLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ReLU             │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ ReLU             │      0 │
│ 10 │ net.model.8  │ Linear           │     66 │
│ 11 │ criterion    │ MSELoss          │      0 │
│ 12 │ train_acc    │ MeanSquaredError │      0 │
│ 13 │ val_acc      │ MeanSquaredError │      0 │
│ 14 │ test_acc     │ MeanSquaredError │      0 │
│ 15 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        315.16it/s loss: 0.0254     
                                   0:00:00                     v_num: 86ai      
                                                               val/acc: 0.016   
                                                               val/acc_best:    
                                                               0.016 train/acc: 
                                                               0.016            
[[36m2022-06-16 12:31:30,919[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:31:31,446[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ReLU_128_64_32/2022-06-16_12-27-37/experiment=NN/H4_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:31:31,450[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:31:31,450[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ReLU_128_64_32/2022-06-16_12-27-37/experiment=NN/H4_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.21019525825977325    │
│         test/loss         │    0.21019525825977325    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 816.40it/s 
[[36m2022-06-16 12:31:31,914[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.169 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.210 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.210 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.210 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.340 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.340 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▅▃▂▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▅▃▂▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.2102
wandb:           test/loss 0.2102
wandb:           train/acc 0.01604
wandb:          train/loss 0.01604
wandb: trainer/global_step 62500
wandb:             val/acc 0.01619
wandb:        val/acc_best 0.01595
wandb:            val/loss 0.01619
wandb: 
wandb: Synced H4_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1qvh86ai
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_122739-1qvh86ai/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7efea5caa1f0>
True
[[36m2022-06-16 12:31:41,719[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ReLU_128_64_32/2022-06-16_12-27-37/experiment=NN/H4_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:31:43,710[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:31:43,710[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:31:44,768[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:31:44,768[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H5_ReLU_128_64_32                                                 
│       net:                                                                    
│         _target_: src.models.components.NN_net.H5_ReLU                        
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H5_ReLU_128_64_32                                               
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H5_ReLU_128_64_32                                                   
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H5_ReLU_128_64_32                                                       
[[36m2022-06-16 12:31:44,798[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:31:44,800[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:31:44,982[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpxnu6nrqf[0m
[[36m2022-06-16 12:31:44,985[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpxnu6nrqf/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:31:44,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:31:44,992[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:31:44,993[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:31:44,993[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:31:44,994[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_123145-mfnz774o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_ReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/mfnz774o
[[36m2022-06-16 12:31:49,693[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:31:49,723[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:31:49,727[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:31:52,574[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H5_ReLU          │ 25.4 K │
│ 1  │ net.model    │ Sequential       │ 25.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ReLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ReLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ReLU             │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ ReLU             │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ ReLU             │      0 │
│ 12 │ net.model.10 │ Linear           │     66 │
│ 13 │ criterion    │ MSELoss          │      0 │
│ 14 │ train_acc    │ MeanSquaredError │      0 │
│ 15 │ val_acc      │ MeanSquaredError │      0 │
│ 16 │ test_acc     │ MeanSquaredError │      0 │
│ 17 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        292.62it/s loss: 0.0154     
                                   0:00:00                     v_num: 774o      
                                                               val/acc: 0.013   
                                                               val/acc_best:    
                                                               0.012 train/acc: 
                                                               0.013            
[[36m2022-06-16 12:35:51,894[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:35:52,448[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ReLU_128_64_32/2022-06-16_12-31-43/experiment=NN/H5_ReLU.yaml/checkpoints/epoch_002.ckpt[0m
[[36m2022-06-16 12:35:52,451[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:35:52,452[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ReLU_128_64_32/2022-06-16_12-31-43/experiment=NN/H5_ReLU.yaml/checkpoints/epoch_002.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.2580607533454895     │
│         test/loss         │    0.2580607533454895     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 799.36it/s 
[[36m2022-06-16 12:35:52,929[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.399 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.404 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc ██▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss ██▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 2
wandb:            test/acc 0.25806
wandb:           test/loss 0.25806
wandb:           train/acc 0.01314
wandb:          train/loss 0.01314
wandb: trainer/global_step 62500
wandb:             val/acc 0.01329
wandb:        val/acc_best 0.01223
wandb:            val/loss 0.01329
wandb: 
wandb: Synced H5_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/mfnz774o
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_123145-mfnz774o/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f8d00a5a370>
True
[[36m2022-06-16 12:35:57,710[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ReLU_128_64_32/2022-06-16_12-31-43/experiment=NN/H5_ReLU.yaml/checkpoints/epoch_002.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:35:59,696[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:35:59,696[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H10_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:36:00,762[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:36:00,762[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H10_ReLU_128_64_32                                                
│       net:                                                                    
│         _target_: src.models.components.NN_net.H10_ReLU                       
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H10_ReLU_128_64_32                                              
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H10_ReLU_128_64_32                                                  
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H10_ReLU_128_64_32                                                      
[[36m2022-06-16 12:36:00,792[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:36:00,795[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:36:00,983[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpwhbr0o6r[0m
[[36m2022-06-16 12:36:00,983[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpwhbr0o6r/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:36:00,990[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:36:00,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:36:00,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:36:00,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:36:00,992[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_123601-2ax59uyk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H10_ReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/2ax59uyk
[[36m2022-06-16 12:36:06,735[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:36:06,767[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:36:06,772[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:36:09,414[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H10_ReLU         │ 30.7 K │
│ 1  │ net.model    │ Sequential       │ 30.7 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ReLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ReLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ReLU             │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ ReLU             │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ ReLU             │      0 │
│ 12 │ net.model.10 │ Linear           │  1.1 K │
│ 13 │ net.model.11 │ ReLU             │      0 │
│ 14 │ net.model.12 │ Linear           │  1.1 K │
│ 15 │ net.model.13 │ ReLU             │      0 │
│ 16 │ net.model.14 │ Linear           │  1.1 K │
│ 17 │ net.model.15 │ ReLU             │      0 │
│ 18 │ net.model.16 │ Linear           │  1.1 K │
│ 19 │ net.model.17 │ ReLU             │      0 │
│ 20 │ net.model.18 │ Linear           │  1.1 K │
│ 21 │ net.model.19 │ ReLU             │      0 │
│ 22 │ net.model.20 │ Linear           │     66 │
│ 23 │ criterion    │ MSELoss          │      0 │
│ 24 │ train_acc    │ MeanSquaredError │      0 │
│ 25 │ val_acc      │ MeanSquaredError │      0 │
│ 26 │ test_acc     │ MeanSquaredError │      0 │
│ 27 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 30.7 K                                                        
Non-trainable params: 0                                                         
Total params: 30.7 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:03 •        223.28it/s loss: 0.022      
                                   0:00:00                     v_num: 9uyk      
                                                               val/acc: 0.025   
                                                               val/acc_best:    
                                                               0.018 train/acc: 
                                                               0.024            
[[36m2022-06-16 12:41:20,167[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:41:20,768[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H10_ReLU_128_64_32/2022-06-16_12-35-59/experiment=NN/H10_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:41:20,774[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:41:20,775[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H10_ReLU_128_64_32/2022-06-16_12-35-59/experiment=NN/H10_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.5824992060661316     │
│         test/loss         │    0.5824992060661316     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 733.59it/s 
[[36m2022-06-16 12:41:21,255[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.395 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.397 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.397 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.5825
wandb:           test/loss 0.5825
wandb:           train/acc 0.02418
wandb:          train/loss 0.02418
wandb: trainer/global_step 62500
wandb:             val/acc 0.02525
wandb:        val/acc_best 0.0177
wandb:            val/loss 0.02525
wandb: 
wandb: Synced H10_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/2ax59uyk
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_123601-2ax59uyk/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f97f97ce460>
True
[[36m2022-06-16 12:41:26,742[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H10_ReLU_128_64_32/2022-06-16_12-35-59/experiment=NN/H10_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:41:28,734[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:41:28,734[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H15_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:41:29,792[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:41:29,792[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H15_ReLU_128_64_32                                                
│       net:                                                                    
│         _target_: src.models.components.NN_net.H15_ReLU                       
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H15_ReLU_128_64_32                                              
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H15_ReLU_128_64_32                                                  
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H15_ReLU_128_64_32                                                      
[[36m2022-06-16 12:41:29,826[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:41:29,829[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:41:30,015[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpui62oj6v[0m
[[36m2022-06-16 12:41:30,015[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpui62oj6v/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:41:30,022[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:41:30,023[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:41:30,024[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:41:30,024[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:41:30,024[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_124130-4of2l252
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H15_ReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/4of2l252
[[36m2022-06-16 12:41:36,488[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:41:36,514[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:41:36,515[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:41:36,519[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:41:39,273[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H15_ReLU         │ 36.0 K │
│ 1  │ net.model    │ Sequential       │ 36.0 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ReLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ReLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ReLU             │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ ReLU             │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ ReLU             │      0 │
│ 12 │ net.model.10 │ Linear           │  1.1 K │
│ 13 │ net.model.11 │ ReLU             │      0 │
│ 14 │ net.model.12 │ Linear           │  1.1 K │
│ 15 │ net.model.13 │ ReLU             │      0 │
│ 16 │ net.model.14 │ Linear           │  1.1 K │
│ 17 │ net.model.15 │ ReLU             │      0 │
│ 18 │ net.model.16 │ Linear           │  1.1 K │
│ 19 │ net.model.17 │ ReLU             │      0 │
│ 20 │ net.model.18 │ Linear           │  1.1 K │
│ 21 │ net.model.19 │ ReLU             │      0 │
│ 22 │ net.model.20 │ Linear           │  1.1 K │
│ 23 │ net.model.21 │ ReLU             │      0 │
│ 24 │ net.model.22 │ Linear           │  1.1 K │
│ 25 │ net.model.23 │ ReLU             │      0 │
│ 26 │ net.model.24 │ Linear           │  1.1 K │
│ 27 │ net.model.25 │ ReLU             │      0 │
│ 28 │ net.model.26 │ Linear           │  1.1 K │
│ 29 │ net.model.27 │ ReLU             │      0 │
│ 30 │ net.model.28 │ Linear           │  1.1 K │
│ 31 │ net.model.29 │ ReLU             │      0 │
│ 32 │ net.model.30 │ Linear           │     66 │
│ 33 │ criterion    │ MSELoss          │      0 │
│ 34 │ train_acc    │ MeanSquaredError │      0 │
│ 35 │ val_acc      │ MeanSquaredError │      0 │
│ 36 │ test_acc     │ MeanSquaredError │      0 │
│ 37 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 36.0 K                                                        
Non-trainable params: 0                                                         
Total params: 36.0 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:03 •        183.58it/s loss: 0.0489     
                                   0:00:00                     v_num: l252      
                                                               val/acc: 0.027   
                                                               val/acc_best:    
                                                               0.019 train/acc: 
                                                               0.029            
[[36m2022-06-16 12:48:06,785[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:48:07,498[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H15_ReLU_128_64_32/2022-06-16_12-41-28/experiment=NN/H15_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:48:07,505[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:48:07,506[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H15_ReLU_128_64_32/2022-06-16_12-41-28/experiment=NN/H15_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │     1.150724172592163     │
│         test/loss         │     1.150724172592163     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 662.78it/s 
[[36m2022-06-16 12:48:08,067[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.396 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 1.15072
wandb:           test/loss 1.15072
wandb:           train/acc 0.02944
wandb:          train/loss 0.02944
wandb: trainer/global_step 62500
wandb:             val/acc 0.02747
wandb:        val/acc_best 0.01872
wandb:            val/loss 0.02747
wandb: 
wandb: Synced H15_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/4of2l252
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_124130-4of2l252/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fcd759051c0>
True
[[36m2022-06-16 12:48:13,348[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H15_ReLU_128_64_32/2022-06-16_12-41-28/experiment=NN/H15_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:48:15,341[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:48:15,341[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_SiLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:48:16,413[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:48:16,413[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H3_SiLU_128_64_32                                                 
│       net:                                                                    
│         _target_: src.models.components.NN_net.H3_SiLU                        
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H3_SiLU_128_64_32                                               
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H3_SiLU_128_64_32                                                   
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H3_SiLU_128_64_32                                                       
[[36m2022-06-16 12:48:16,442[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:48:16,445[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:48:16,625[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpp1oinyaz[0m
[[36m2022-06-16 12:48:16,626[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpp1oinyaz/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:48:16,632[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:48:16,633[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:48:16,633[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:48:16,634[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:48:16,634[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_124817-2ql9x1k6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_SiLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/2ql9x1k6
[[36m2022-06-16 12:48:21,377[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:48:21,406[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:48:21,407[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:48:21,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:48:24,110[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H3_SiLU          │ 23.3 K │
│ 1  │ net.model    │ Sequential       │ 23.3 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ SiLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ SiLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ SiLU             │      0 │
│ 8  │ net.model.6  │ Linear           │     66 │
│ 9  │ criterion    │ MSELoss          │      0 │
│ 10 │ train_acc    │ MeanSquaredError │      0 │
│ 11 │ val_acc      │ MeanSquaredError │      0 │
│ 12 │ test_acc     │ MeanSquaredError │      0 │
│ 13 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        338.07it/s loss: 0.0409     
                                   0:00:00                     v_num: x1k6      
                                                               val/acc: 0.031   
                                                               val/acc_best:    
                                                               0.026 train/acc: 
                                                               0.033            
[[36m2022-06-16 12:51:52,165[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:51:52,802[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_SiLU_128_64_32/2022-06-16_12-48-15/experiment=NN/H3_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:51:52,805[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:51:52,805[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_SiLU_128_64_32/2022-06-16_12-48-15/experiment=NN/H3_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.1421467661857605     │
│         test/loss         │    0.1421467661857605     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 838.01it/s 
[[36m2022-06-16 12:51:53,273[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.14215
wandb:           test/loss 0.14215
wandb:           train/acc 0.03286
wandb:          train/loss 0.03286
wandb: trainer/global_step 62500
wandb:             val/acc 0.03125
wandb:        val/acc_best 0.02564
wandb:            val/loss 0.03125
wandb: 
wandb: Synced H3_SiLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/2ql9x1k6
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_124817-2ql9x1k6/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7eff11ac5f70>
True
[[36m2022-06-16 12:51:58,284[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_SiLU_128_64_32/2022-06-16_12-48-15/experiment=NN/H3_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:52:00,272[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:52:00,272[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_SiLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:52:01,328[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:52:01,328[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H4_SiLU_128_64_32                                                 
│       net:                                                                    
│         _target_: src.models.components.NN_net.H4_SiLU                        
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H4_SiLU_128_64_32                                               
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H4_SiLU_128_64_32                                                   
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H4_SiLU_128_64_32                                                       
[[36m2022-06-16 12:52:01,358[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:52:01,360[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:52:01,550[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpv72kakih[0m
[[36m2022-06-16 12:52:01,550[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpv72kakih/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:52:01,557[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:52:01,558[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:52:01,558[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:52:01,559[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:52:01,559[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_125202-1k9qjw75
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_SiLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1k9qjw75
[[36m2022-06-16 12:52:09,673[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:52:09,705[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:52:09,711[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:52:12,363[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H4_SiLU          │ 24.4 K │
│ 1  │ net.model    │ Sequential       │ 24.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ SiLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ SiLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ SiLU             │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ SiLU             │      0 │
│ 10 │ net.model.8  │ Linear           │     66 │
│ 11 │ criterion    │ MSELoss          │      0 │
│ 12 │ train_acc    │ MeanSquaredError │      0 │
│ 13 │ val_acc      │ MeanSquaredError │      0 │
│ 14 │ test_acc     │ MeanSquaredError │      0 │
│ 15 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        316.59it/s loss: 0.0285     
                                   0:00:00                     v_num: jw75      
                                                               val/acc: 0.024   
                                                               val/acc_best:    
                                                               0.023 train/acc: 
                                                               0.025            
[[36m2022-06-16 12:55:56,743[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:55:57,273[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_SiLU_128_64_32/2022-06-16_12-52-00/experiment=NN/H4_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:55:57,276[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:55:57,276[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_SiLU_128_64_32/2022-06-16_12-52-00/experiment=NN/H4_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.1403432935476303     │
│         test/loss         │    0.1403432935476303     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 821.29it/s 
[[36m2022-06-16 12:55:57,747[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.198 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.198 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.202 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.207 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.387 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▃▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▃▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.14034
wandb:           test/loss 0.14034
wandb:           train/acc 0.02529
wandb:          train/loss 0.02529
wandb: trainer/global_step 62500
wandb:             val/acc 0.02368
wandb:        val/acc_best 0.02278
wandb:            val/loss 0.02368
wandb: 
wandb: Synced H4_SiLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1k9qjw75
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_125202-1k9qjw75/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f25e14371c0>
True
[[36m2022-06-16 12:56:02,395[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_SiLU_128_64_32/2022-06-16_12-52-00/experiment=NN/H4_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:56:04,386[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:56:04,386[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_SiLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:56:05,472[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:56:05,472[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H5_SiLU_128_64_32                                                 
│       net:                                                                    
│         _target_: src.models.components.NN_net.H5_SiLU                        
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H5_SiLU_128_64_32                                               
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H5_SiLU_128_64_32                                                   
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H5_SiLU_128_64_32                                                       
[[36m2022-06-16 12:56:05,502[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:56:05,505[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:56:05,700[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmppp2m_bzq[0m
[[36m2022-06-16 12:56:05,700[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmppp2m_bzq/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:56:05,706[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:56:05,707[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:56:05,708[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:56:05,708[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:56:05,709[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_125606-hifbh0jw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_SiLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/hifbh0jw
[[36m2022-06-16 12:56:10,500[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:56:10,526[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:56:10,527[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:56:10,531[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:56:13,184[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H5_SiLU          │ 25.4 K │
│ 1  │ net.model    │ Sequential       │ 25.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ SiLU             │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ SiLU             │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ SiLU             │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ SiLU             │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ SiLU             │      0 │
│ 12 │ net.model.10 │ Linear           │     66 │
│ 13 │ criterion    │ MSELoss          │      0 │
│ 14 │ train_acc    │ MeanSquaredError │      0 │
│ 15 │ val_acc      │ MeanSquaredError │      0 │
│ 16 │ test_acc     │ MeanSquaredError │      0 │
│ 17 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        300.20it/s loss: 0.0323     
                                   0:00:00                     v_num: h0jw      
                                                               val/acc: 0.033   
                                                               val/acc_best:    
                                                               0.028 train/acc: 
                                                               0.033            
[[36m2022-06-16 13:00:12,781[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:00:13,384[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_SiLU_128_64_32/2022-06-16_12-56-04/experiment=NN/H5_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:00:13,387[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:00:13,388[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_SiLU_128_64_32/2022-06-16_12-56-04/experiment=NN/H5_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.11632352322340012    │
│         test/loss         │    0.11632352322340012    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 826.02it/s 
[[36m2022-06-16 13:00:13,845[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.396 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.401 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▆▄▂▂▃▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁
wandb:        val/acc_best █▆▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▆▄▂▂▃▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.11632
wandb:           test/loss 0.11632
wandb:           train/acc 0.03269
wandb:          train/loss 0.03269
wandb: trainer/global_step 62500
wandb:             val/acc 0.0326
wandb:        val/acc_best 0.02838
wandb:            val/loss 0.0326
wandb: 
wandb: Synced H5_SiLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/hifbh0jw
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_125606-hifbh0jw/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f622eb4f250>
True
[[36m2022-06-16 13:00:17,872[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_SiLU_128_64_32/2022-06-16_12-56-04/experiment=NN/H5_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:00:19,863[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:00:19,863[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_LeakyReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:00:20,909[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:00:20,909[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H3_LeakyReLU_128_64_32                                            
│       net:                                                                    
│         _target_: src.models.components.NN_net.H3_LeakyReLU                   
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H3_LeakyReLU_128_64_32                                          
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H3_LeakyReLU_128_64_32                                              
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H3_LeakyReLU_128_64_32                                                  
[[36m2022-06-16 13:00:20,938[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:00:20,941[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:00:21,130[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpy7c2jitq[0m
[[36m2022-06-16 13:00:21,131[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpy7c2jitq/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:00:21,137[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:00:21,138[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:00:21,138[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:00:21,139[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:00:21,139[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_130022-3ld4thqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_LeakyReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/3ld4thqq
[[36m2022-06-16 13:00:26,030[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:00:26,058[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:00:26,059[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:00:26,063[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:00:28,704[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H3_LeakyReLU     │ 23.3 K │
│ 1  │ net.model    │ Sequential       │ 23.3 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ LeakyReLU        │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ LeakyReLU        │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ LeakyReLU        │      0 │
│ 8  │ net.model.6  │ Linear           │     66 │
│ 9  │ criterion    │ MSELoss          │      0 │
│ 10 │ train_acc    │ MeanSquaredError │      0 │
│ 11 │ val_acc      │ MeanSquaredError │      0 │
│ 12 │ test_acc     │ MeanSquaredError │      0 │
│ 13 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        341.63it/s loss: 0.0178     
                                   0:00:00                     v_num: thqq      
                                                               val/acc: 0.025   
                                                               val/acc_best:    
                                                               0.021 train/acc: 
                                                               0.022            
[[36m2022-06-16 13:03:58,885[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:03:59,536[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_LeakyReLU_128_64_32/2022-06-16_13-00-19/experiment=NN/H3_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:03:59,539[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:03:59,540[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_LeakyReLU_128_64_32/2022-06-16_13-00-19/experiment=NN/H3_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.24686571955680847    │
│         test/loss         │    0.24686571955680847    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 854.26it/s 
[[36m2022-06-16 13:03:59,997[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▂▃▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▂▃▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.24687
wandb:           test/loss 0.24687
wandb:           train/acc 0.0222
wandb:          train/loss 0.0222
wandb: trainer/global_step 62500
wandb:             val/acc 0.02464
wandb:        val/acc_best 0.02065
wandb:            val/loss 0.02464
wandb: 
wandb: Synced H3_LeakyReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/3ld4thqq
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_130022-3ld4thqq/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f96d59bab20>
True
[[36m2022-06-16 13:04:06,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_LeakyReLU_128_64_32/2022-06-16_13-00-19/experiment=NN/H3_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:04:08,395[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:04:08,395[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_LeakyReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:04:09,468[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:04:09,469[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H4_LeakyReLU_128_64_32                                            
│       net:                                                                    
│         _target_: src.models.components.NN_net.H4_LeakyReLU                   
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H4_LeakyReLU_128_64_32                                          
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H4_LeakyReLU_128_64_32                                              
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H4_LeakyReLU_128_64_32                                                  
[[36m2022-06-16 13:04:09,498[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:04:09,500[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:04:09,684[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp99rfpttd[0m
[[36m2022-06-16 13:04:09,686[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp99rfpttd/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:04:09,693[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:04:09,694[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:04:09,694[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:04:09,695[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:04:09,695[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_130410-1lk2sjfz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_LeakyReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1lk2sjfz
[[36m2022-06-16 13:04:15,392[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:04:15,418[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:04:15,419[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:04:15,423[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:04:18,098[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H4_LeakyReLU     │ 24.4 K │
│ 1  │ net.model    │ Sequential       │ 24.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ LeakyReLU        │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ LeakyReLU        │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ LeakyReLU        │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ LeakyReLU        │      0 │
│ 10 │ net.model.8  │ Linear           │     66 │
│ 11 │ criterion    │ MSELoss          │      0 │
│ 12 │ train_acc    │ MeanSquaredError │      0 │
│ 13 │ val_acc      │ MeanSquaredError │      0 │
│ 14 │ test_acc     │ MeanSquaredError │      0 │
│ 15 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        313.06it/s loss: 0.0244     
                                   0:00:00                     v_num: sjfz      
                                                               val/acc: 0.017   
                                                               val/acc_best:    
                                                               0.017 train/acc: 
                                                               0.018            
[[36m2022-06-16 13:08:01,888[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:08:02,498[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_LeakyReLU_128_64_32/2022-06-16_13-04-08/experiment=NN/H4_LeakyReLU.yaml/checkpoints/epoch_001.ckpt[0m
[[36m2022-06-16 13:08:02,501[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:08:02,502[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_LeakyReLU_128_64_32/2022-06-16_13-04-08/experiment=NN/H4_LeakyReLU.yaml/checkpoints/epoch_001.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.2511289417743683     │
│         test/loss         │    0.2511289417743683     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 828.34it/s 
[[36m2022-06-16 13:08:02,969[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▄▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▄▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 1
wandb:            test/acc 0.25113
wandb:           test/loss 0.25113
wandb:           train/acc 0.01759
wandb:          train/loss 0.01759
wandb: trainer/global_step 62500
wandb:             val/acc 0.01666
wandb:        val/acc_best 0.01666
wandb:            val/loss 0.01666
wandb: 
wandb: Synced H4_LeakyReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1lk2sjfz
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_130410-1lk2sjfz/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4b431baca0>
True
[[36m2022-06-16 13:08:07,626[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_LeakyReLU_128_64_32/2022-06-16_13-04-08/experiment=NN/H4_LeakyReLU.yaml/checkpoints/epoch_001.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:08:09,606[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:08:09,606[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_LeakyReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:08:10,722[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:08:10,722[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H5_LeakyReLU_128_64_32                                            
│       net:                                                                    
│         _target_: src.models.components.NN_net.H5_LeakyReLU                   
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H5_LeakyReLU_128_64_32                                          
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H5_LeakyReLU_128_64_32                                              
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H5_LeakyReLU_128_64_32                                                  
[[36m2022-06-16 13:08:10,752[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:08:10,755[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:08:10,942[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpa68kinxp[0m
[[36m2022-06-16 13:08:10,943[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpa68kinxp/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:08:10,949[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:08:10,950[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:08:10,950[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:08:10,951[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:08:10,951[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_130811-2ai36gr3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_LeakyReLU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/2ai36gr3
[[36m2022-06-16 13:08:15,654[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:08:15,683[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:08:15,687[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:08:18,334[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H5_LeakyReLU     │ 25.4 K │
│ 1  │ net.model    │ Sequential       │ 25.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ LeakyReLU        │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ LeakyReLU        │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ LeakyReLU        │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ LeakyReLU        │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ LeakyReLU        │      0 │
│ 12 │ net.model.10 │ Linear           │     66 │
│ 13 │ criterion    │ MSELoss          │      0 │
│ 14 │ train_acc    │ MeanSquaredError │      0 │
│ 15 │ val_acc      │ MeanSquaredError │      0 │
│ 16 │ test_acc     │ MeanSquaredError │      0 │
│ 17 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        295.45it/s loss: 0.0134     
                                   0:00:00                     v_num: 6gr3      
                                                               val/acc: 0.018   
                                                               val/acc_best:    
                                                               0.017 train/acc: 
                                                               0.018            
[[36m2022-06-16 13:12:17,558[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:12:18,138[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_LeakyReLU_128_64_32/2022-06-16_13-08-09/experiment=NN/H5_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:12:18,142[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:12:18,143[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_LeakyReLU_128_64_32/2022-06-16_13-08-09/experiment=NN/H5_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.2136443853378296     │
│         test/loss         │    0.2136443853378296     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 815.02it/s 
[[36m2022-06-16 13:12:18,614[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.393 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.21364
wandb:           test/loss 0.21364
wandb:           train/acc 0.0176
wandb:          train/loss 0.0176
wandb: trainer/global_step 62500
wandb:             val/acc 0.0176
wandb:        val/acc_best 0.01713
wandb:            val/loss 0.0176
wandb: 
wandb: Synced H5_LeakyReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/2ai36gr3
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_130811-2ai36gr3/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fa8c2e08130>
True
[[36m2022-06-16 13:12:22,538[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_LeakyReLU_128_64_32/2022-06-16_13-08-09/experiment=NN/H5_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:12:24,522[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:12:24,522[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_ELU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:12:25,576[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:12:25,577[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H3_ELU_128_64_32                                                  
│       net:                                                                    
│         _target_: src.models.components.NN_net.H3_ELU                         
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H3_ELU_128_64_32                                                
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H3_ELU_128_64_32                                                    
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H3_ELU_128_64_32                                                        
[[36m2022-06-16 13:12:25,606[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:12:25,609[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:12:25,797[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpt_2e_j1g[0m
[[36m2022-06-16 13:12:25,798[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpt_2e_j1g/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:12:25,804[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:12:25,805[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:12:25,805[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:12:25,806[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:12:25,806[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_131226-22c2yeeu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_ELU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/22c2yeeu
[[36m2022-06-16 13:12:30,549[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:12:30,581[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:12:30,581[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:12:30,587[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:12:33,189[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H3_ELU           │ 23.3 K │
│ 1  │ net.model    │ Sequential       │ 23.3 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ELU              │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ELU              │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ELU              │      0 │
│ 8  │ net.model.6  │ Linear           │     66 │
│ 9  │ criterion    │ MSELoss          │      0 │
│ 10 │ train_acc    │ MeanSquaredError │      0 │
│ 11 │ val_acc      │ MeanSquaredError │      0 │
│ 12 │ test_acc     │ MeanSquaredError │      0 │
│ 13 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        335.34it/s loss: 0.0291     
                                   0:00:00                     v_num: yeeu      
                                                               val/acc: 0.025   
                                                               val/acc_best:    
                                                               0.022 train/acc: 
                                                               0.027            
[[36m2022-06-16 13:16:03,336[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:16:03,869[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ELU_128_64_32/2022-06-16_13-12-24/experiment=NN/H3_ELU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:16:03,872[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:16:03,872[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ELU_128_64_32/2022-06-16_13-12-24/experiment=NN/H3_ELU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.11063409596681595    │
│         test/loss         │    0.11063409596681595    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 854.92it/s 
[[36m2022-06-16 13:16:04,333[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.396 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▃▂▃▂▂▂▃▂▁▂▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▃▂▃▂▂▂▃▂▁▂▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.11063
wandb:           test/loss 0.11063
wandb:           train/acc 0.02672
wandb:          train/loss 0.02672
wandb: trainer/global_step 62500
wandb:             val/acc 0.02538
wandb:        val/acc_best 0.02246
wandb:            val/loss 0.02538
wandb: 
wandb: Synced H3_ELU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/22c2yeeu
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_131226-22c2yeeu/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7ff31dbdcdf0>
True
[[36m2022-06-16 13:16:07,913[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ELU_128_64_32/2022-06-16_13-12-24/experiment=NN/H3_ELU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:16:09,893[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:16:09,894[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_ELU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:16:10,937[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:16:10,938[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H4_ELU_128_64_32                                                  
│       net:                                                                    
│         _target_: src.models.components.NN_net.H4_ELU                         
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H4_ELU_128_64_32                                                
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H4_ELU_128_64_32                                                    
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H4_ELU_128_64_32                                                        
[[36m2022-06-16 13:16:10,967[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:16:10,970[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:16:11,156[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp9yjj88t4[0m
[[36m2022-06-16 13:16:11,156[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp9yjj88t4/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:16:11,162[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:16:11,163[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:16:11,163[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:16:11,164[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:16:11,164[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_131612-zu9t9z2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_ELU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/zu9t9z2c
[[36m2022-06-16 13:16:15,916[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:16:15,942[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:16:15,943[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:16:15,947[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:16:18,581[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H4_ELU           │ 24.4 K │
│ 1  │ net.model    │ Sequential       │ 24.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ELU              │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ELU              │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ELU              │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ ELU              │      0 │
│ 10 │ net.model.8  │ Linear           │     66 │
│ 11 │ criterion    │ MSELoss          │      0 │
│ 12 │ train_acc    │ MeanSquaredError │      0 │
│ 13 │ val_acc      │ MeanSquaredError │      0 │
│ 14 │ test_acc     │ MeanSquaredError │      0 │
│ 15 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        311.73it/s loss: 0.0156     
                                   0:00:00                     v_num: 9z2c      
                                                               val/acc: 0.021   
                                                               val/acc_best:    
                                                               0.02 train/acc:  
                                                               0.022            
[[36m2022-06-16 13:20:00,918[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:20:01,520[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ELU_128_64_32/2022-06-16_13-16-09/experiment=NN/H4_ELU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:20:01,523[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:20:01,524[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ELU_128_64_32/2022-06-16_13-16-09/experiment=NN/H4_ELU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.10119711607694626    │
│         test/loss         │    0.10119711607694626    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 829.62it/s 
[[36m2022-06-16 13:20:01,994[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.395 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.398 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▅▃▂▃▂▃▃▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▅▃▂▃▂▃▃▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.1012
wandb:           test/loss 0.1012
wandb:           train/acc 0.02172
wandb:          train/loss 0.02172
wandb: trainer/global_step 62500
wandb:             val/acc 0.0212
wandb:        val/acc_best 0.02023
wandb:            val/loss 0.0212
wandb: 
wandb: Synced H4_ELU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/zu9t9z2c
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_131612-zu9t9z2c/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4fbef3d280>
True
[[36m2022-06-16 13:20:06,227[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ELU_128_64_32/2022-06-16_13-16-09/experiment=NN/H4_ELU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:20:08,207[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:20:08,207[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_ELU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:20:09,241[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:20:09,242[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H5_ELU_128_64_32                                                  
│       net:                                                                    
│         _target_: src.models.components.NN_net.H5_ELU                         
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H5_ELU_128_64_32                                                
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H5_ELU_128_64_32                                                    
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H5_ELU_128_64_32                                                        
[[36m2022-06-16 13:20:09,271[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:20:09,274[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:20:09,462[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpkkds2k1f[0m
[[36m2022-06-16 13:20:09,463[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpkkds2k1f/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:20:09,469[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:20:09,470[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:20:09,471[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:20:09,471[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:20:09,471[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_132010-1cl51qao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_ELU_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1cl51qao
[[36m2022-06-16 13:20:14,167[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:20:14,194[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:20:14,195[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:20:14,199[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:20:16,818[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H5_ELU           │ 25.4 K │
│ 1  │ net.model    │ Sequential       │ 25.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ ELU              │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ ELU              │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ ELU              │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ ELU              │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ ELU              │      0 │
│ 12 │ net.model.10 │ Linear           │     66 │
│ 13 │ criterion    │ MSELoss          │      0 │
│ 14 │ train_acc    │ MeanSquaredError │      0 │
│ 15 │ val_acc      │ MeanSquaredError │      0 │
│ 16 │ test_acc     │ MeanSquaredError │      0 │
│ 17 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        299.47it/s loss: 0.0129     
                                   0:00:00                     v_num: 1qao      
                                                               val/acc: 0.02    
                                                               val/acc_best:    
                                                               0.017 train/acc: 
                                                               0.019            
[[36m2022-06-16 13:24:14,184[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:24:14,695[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ELU_128_64_32/2022-06-16_13-20-08/experiment=NN/H5_ELU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:24:14,698[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:24:14,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ELU_128_64_32/2022-06-16_13-20-08/experiment=NN/H5_ELU.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    0.07156991958618164    │
│         test/loss         │    0.07156991958618164    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 813.23it/s 
[[36m2022-06-16 13:24:15,161[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▄▅▃▄▃▃▃▂▃▂▂▂▂▂▃▂▂▁▂▁▂▁▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁
wandb:        val/acc_best █▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▄▅▃▄▃▃▃▂▃▂▂▂▂▂▃▂▂▁▂▁▂▁▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.07157
wandb:           test/loss 0.07157
wandb:           train/acc 0.01926
wandb:          train/loss 0.01926
wandb: trainer/global_step 62500
wandb:             val/acc 0.01954
wandb:        val/acc_best 0.01683
wandb:            val/loss 0.01954
wandb: 
wandb: Synced H5_ELU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1cl51qao
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_132010-1cl51qao/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fcf281704c0>
True
[[36m2022-06-16 13:24:19,284[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ELU_128_64_32/2022-06-16_13-20-08/experiment=NN/H5_ELU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:24:21,269[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:24:21,269[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_Softshrink.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:24:22,308[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:24:22,309[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H3_Softshrink_128_64_32                                           
│       net:                                                                    
│         _target_: src.models.components.NN_net.H3_Softshrink                  
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H3_Softshrink_128_64_32                                         
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H3_Softshrink_128_64_32                                             
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H3_Softshrink_128_64_32                                                 
[[36m2022-06-16 13:24:22,339[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:24:22,342[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:24:22,535[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpzw1hajxn[0m
[[36m2022-06-16 13:24:22,535[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpzw1hajxn/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:24:22,541[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:24:22,543[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:24:22,543[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:24:22,543[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:24:22,544[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_132423-33p0yrbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_Softshrink_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/33p0yrbc
[[36m2022-06-16 13:24:27,263[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:24:27,290[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:24:27,291[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:24:27,295[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:24:30,037[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H3_Softshrink    │ 23.3 K │
│ 1  │ net.model    │ Sequential       │ 23.3 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ Softshrink       │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ Softshrink       │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ Softshrink       │      0 │
│ 8  │ net.model.6  │ Linear           │     66 │
│ 9  │ criterion    │ MSELoss          │      0 │
│ 10 │ train_acc    │ MeanSquaredError │      0 │
│ 11 │ val_acc      │ MeanSquaredError │      0 │
│ 12 │ test_acc     │ MeanSquaredError │      0 │
│ 13 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        338.98it/s loss: 0.0695     
                                   0:00:00                     v_num: yrbc      
                                                               val/acc: 0.072   
                                                               val/acc_best:    
                                                               0.066 train/acc: 
                                                               0.082            
[[36m2022-06-16 13:27:59,990[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:28:00,610[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_Softshrink_128_64_32/2022-06-16_13-24-21/experiment=NN/H3_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:28:00,613[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:28:00,614[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_Softshrink_128_64_32/2022-06-16_13-24-21/experiment=NN/H3_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    1.8656529188156128     │
│         test/loss         │    1.8656529188156128     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 825.58it/s 
[[36m2022-06-16 13:28:01,077[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 1.86565
wandb:           test/loss 1.86565
wandb:           train/acc 0.08166
wandb:          train/loss 0.08166
wandb: trainer/global_step 62500
wandb:             val/acc 0.07181
wandb:        val/acc_best 0.06642
wandb:            val/loss 0.07181
wandb: 
wandb: Synced H3_Softshrink_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/33p0yrbc
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_132423-33p0yrbc/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fd3c9b43fa0>
True
[[36m2022-06-16 13:28:05,651[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_Softshrink_128_64_32/2022-06-16_13-24-21/experiment=NN/H3_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:28:07,637[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:28:07,637[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_Softshrink.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:28:08,703[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:28:08,703[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H4_Softshrink_128_64_32                                           
│       net:                                                                    
│         _target_: src.models.components.NN_net.H4_Softshrink                  
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H4_Softshrink_128_64_32                                         
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H4_Softshrink_128_64_32                                             
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H4_Softshrink_128_64_32                                                 
[[36m2022-06-16 13:28:08,733[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:28:08,736[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:28:08,926[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpv3ixsbft[0m
[[36m2022-06-16 13:28:08,926[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpv3ixsbft/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:28:08,932[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:28:08,933[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:28:08,934[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:28:08,934[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:28:08,935[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_132809-iivc1rm6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_Softshrink_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/iivc1rm6
[[36m2022-06-16 13:28:13,622[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:28:13,650[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:28:13,651[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:28:13,655[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:28:16,305[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H4_Softshrink    │ 24.4 K │
│ 1  │ net.model    │ Sequential       │ 24.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ Softshrink       │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ Softshrink       │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ Softshrink       │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ Softshrink       │      0 │
│ 10 │ net.model.8  │ Linear           │     66 │
│ 11 │ criterion    │ MSELoss          │      0 │
│ 12 │ train_acc    │ MeanSquaredError │      0 │
│ 13 │ val_acc      │ MeanSquaredError │      0 │
│ 14 │ test_acc     │ MeanSquaredError │      0 │
│ 15 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        311.60it/s loss: 21.3 v_num:
                                   0:00:00                     1rm6 val/acc:    
                                                               20.719           
                                                               val/acc_best:    
                                                               20.717 train/acc:
                                                               20.887           
[[36m2022-06-16 13:32:00,797[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:32:01,386[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_Softshrink_128_64_32/2022-06-16_13-28-07/experiment=NN/H4_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:32:01,389[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:32:01,390[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_Softshrink_128_64_32/2022-06-16_13-28-07/experiment=NN/H4_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    31.097734451293945     │
│         test/loss         │    31.097734451293945     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 799.14it/s 
[[36m2022-06-16 13:32:01,850[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: | 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.409 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.409 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 31.09773
wandb:           test/loss 31.09773
wandb:           train/acc 20.88711
wandb:          train/loss 20.88711
wandb: trainer/global_step 62500
wandb:             val/acc 20.71862
wandb:        val/acc_best 20.71737
wandb:            val/loss 20.71862
wandb: 
wandb: Synced H4_Softshrink_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/iivc1rm6
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_132809-iivc1rm6/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f41a5e9beb0>
True
[[36m2022-06-16 13:32:05,889[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_Softshrink_128_64_32/2022-06-16_13-28-07/experiment=NN/H4_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:32:07,875[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:32:07,875[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_Softshrink.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:32:08,919[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:32:08,919[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.gauss_datamodule.gaussDataModule              
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       batch_size: 64                                                          
│       train_val_test_split:                                                   
│       - 40000                                                                 
│       - 5000                                                                  
│       - 5000                                                                  
│       num_workers: 0                                                          
│       pin_memory: false                                                       
│                                                                               
├── model
│   └── _target_: src.models.gauss_module.gaussLitModule                        
│       lr: 0.001                                                               
│       weight_decay: 0.0005                                                    
│       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
│       name: H5_Softshrink_128_64_32                                           
│       net:                                                                    
│         _target_: src.models.components.NN_net.H5_Softshrink                  
│         input_size: 100                                                       
│         lin1_size: 128                                                        
│         lin2_size: 64                                                         
│         lin3_size: 32                                                         
│         output_size: 2                                                        
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         monitor: val/acc                                                      
│         mode: max                                                             
│         save_top_k: 1                                                         
│         save_last: true                                                       
│         verbose: false                                                        
│         dirpath: checkpoints/                                                 
│         filename: epoch_{epoch:03d}                                           
│         auto_insert_metric_name: false                                        
│       early_stopping:                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                   
│         monitor: val/acc                                                      
│         mode: max                                                             
│         patience: 100                                                         
│         min_delta: 0                                                          
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         project: nngauss_NN                                                   
│         name: H5_Softshrink_128_64_32                                         
│         save_dir: /tmp                                                        
│         offline: false                                                        
│         id: null                                                              
│         log_model: false                                                      
│         prefix: ''                                                            
│         job_type: train                                                       
│         group: ''                                                             
│         tags:                                                                 
│         - gauss                                                               
│         - H5_Softshrink_128_64_32                                             
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       gpus: 1                                                                 
│       min_epochs: 1                                                           
│       max_epochs: 100                                                         
│       resume_from_checkpoint: null                                            
│       gradient_clip_val: 0.5                                                  
│                                                                               
├── original_work_dir
│   └── /home/ryuichi/machine_learning/nngauss                                  
├── data_dir
│   └── /home/ryuichi/machine_learning/nngauss/data/                            
├── print_config
│   └── True                                                                    
├── ignore_warnings
│   └── True                                                                    
├── train
│   └── True                                                                    
├── test
│   └── True                                                                    
├── seed
│   └── 0                                                                       
└── name
    └── H5_Softshrink_128_64_32                                                 
[[36m2022-06-16 13:32:08,949[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:32:08,952[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:32:09,138[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp9imu8t4b[0m
[[36m2022-06-16 13:32:09,139[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp9imu8t4b/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:32:09,145[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:32:09,146[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:32:09,147[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:32:09,147[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:32:09,147[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_133210-3inzguao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_Softshrink_128_64_32
wandb: ⭐️ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: 🚀 View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/3inzguao
[[36m2022-06-16 13:32:13,852[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:32:13,878[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:32:13,879[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:32:13,883[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:32:16,531[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
┏━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name         ┃ Type             ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net          │ H5_Softshrink    │ 25.4 K │
│ 1  │ net.model    │ Sequential       │ 25.4 K │
│ 2  │ net.model.0  │ Linear           │ 12.9 K │
│ 3  │ net.model.1  │ Softshrink       │      0 │
│ 4  │ net.model.2  │ Linear           │  8.3 K │
│ 5  │ net.model.3  │ Softshrink       │      0 │
│ 6  │ net.model.4  │ Linear           │  2.1 K │
│ 7  │ net.model.5  │ Softshrink       │      0 │
│ 8  │ net.model.6  │ Linear           │  1.1 K │
│ 9  │ net.model.7  │ Softshrink       │      0 │
│ 10 │ net.model.8  │ Linear           │  1.1 K │
│ 11 │ net.model.9  │ Softshrink       │      0 │
│ 12 │ net.model.10 │ Linear           │     66 │
│ 13 │ criterion    │ MSELoss          │      0 │
│ 14 │ train_acc    │ MeanSquaredError │      0 │
│ 15 │ val_acc      │ MeanSquaredError │      0 │
│ 16 │ test_acc     │ MeanSquaredError │      0 │
│ 17 │ val_acc_best │ MinMetric        │      0 │
└────┴──────────────┴──────────────────┴────────┘
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  ━━━━━━━━━━━━━━━━ 704/704 0:00:02 •        299.48it/s loss: 20.3 v_num:
                                   0:00:00                     guao val/acc:    
                                                               20.719           
                                                               val/acc_best:    
                                                               20.717 train/acc:
                                                               20.887           
[[36m2022-06-16 13:36:14,297[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:36:14,906[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_Softshrink_128_64_32/2022-06-16_13-32-07/experiment=NN/H5_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:36:14,910[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:36:14,910[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_Softshrink_128_64_32/2022-06-16_13-32-07/experiment=NN/H5_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test/acc          │    29.695537567138672     │
│         test/loss         │    29.695537567138672     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 0:00:00 • 0:00:00 825.27it/s 
[[36m2022-06-16 13:36:15,376[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: | 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            test/acc ▁
wandb:           test/loss ▁
wandb:           train/acc █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val/acc █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        val/acc_best █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 29.69554
wandb:           test/loss 29.69554
wandb:           train/acc 20.88716
wandb:          train/loss 20.88716
wandb: trainer/global_step 62500
wandb:             val/acc 20.71916
wandb:        val/acc_best 20.71712
wandb:            val/loss 20.71916
wandb: 
wandb: Synced H5_Softshrink_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/3inzguao
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_133210-3inzguao/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fec8fb193d0>
True
[[36m2022-06-16 13:36:20,203[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_Softshrink_128_64_32/2022-06-16_13-32-07/experiment=NN/H5_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
