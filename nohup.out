/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:35:11,767[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:35:11,767[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_03[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:35:15,047[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:35:15,048[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       net:                                                                    
â”‚         _target_: src.models.components.gauss_net.SimpleDenseNet_ELU_gauss    
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss                                                      
â”‚         name: Dense_3_Hidden_128_64_32_ELU                                    
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - Dense_3_Hidden_128_64_32_ELU                                        
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ Dense_3_Hidden_128_64_32_ELU                                            
[[36m2022-06-15 09:35:15,087[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:35:15,103[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:35:15,508[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpamza028f[0m
[[36m2022-06-15 09:35:15,509[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpamza028f/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:35:15,517[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:35:15,518[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:35:15,518[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:35:15,519[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:35:15,519[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_093516-6g9js550
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_ELU
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss/runs/6g9js550
[[36m2022-06-15 09:35:21,003[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:35:21,040[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:35:21,041[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:35:21,045[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:35:23,327[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type                     â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ SimpleDenseNet_ELU_gauss â”‚ 23.8 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential               â”‚ 23.8 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear                   â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d              â”‚    256 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ELU                      â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear                   â”‚  8.3 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d              â”‚    128 â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ELU                      â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear                   â”‚  2.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d              â”‚     64 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ELU                      â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear                   â”‚     66 â”‚
â”‚ 12 â”‚ criterion    â”‚ MSELoss                  â”‚      0 â”‚
â”‚ 13 â”‚ train_acc    â”‚ MeanSquaredError         â”‚      0 â”‚
â”‚ 14 â”‚ val_acc      â”‚ MeanSquaredError         â”‚      0 â”‚
â”‚ 15 â”‚ test_acc     â”‚ MeanSquaredError         â”‚      0 â”‚
â”‚ 16 â”‚ val_acc_best â”‚ MinMetric                â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        284.09it/s loss: 0.905      
                                   0:00:00                     v_num: s550      
                                                               val/acc: 0.479   
                                                               val/acc_best:    
                                                               0.384 train/acc: 
                                                               0.858            
[[36m2022-06-15 09:39:34,654[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:39:34,656[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_ELU/2022-06-15_09-35-11/experiment=gauss_03/checkpoints/epoch_002.ckpt[0m
[[36m2022-06-15 09:39:34,660[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:39:34,660[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_ELU/2022-06-15_09-35-11/experiment=gauss_03/checkpoints/epoch_002.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.7315894365310669     â”‚
â”‚         test/loss         â”‚    0.7315894365310669     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 783.65it/s 
[[36m2022-06-15 09:39:35,053[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.299 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–‡â–ˆâ–†â–„â–‡â–„â–‚â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–„â–â–‚â–â–‚â–â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–„â–â–‚â–‚â–ƒ
wandb:        val/acc_best â–ˆâ–‡â–†â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–‡â–ˆâ–†â–„â–‡â–„â–‚â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–„â–â–‚â–â–‚â–â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–„â–â–‚â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:               epoch 2
wandb:            test/acc 0.73159
wandb:           test/loss 0.73159
wandb:           train/acc 0.85764
wandb:          train/loss 0.85764
wandb: trainer/global_step 62500
wandb:             val/acc 0.47857
wandb:        val/acc_best 0.38381
wandb:            val/loss 0.47857
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_ELU: https://wandb.ai/rshimogawa/nngauss/runs/6g9js550
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_093516-6g9js550/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4f2dfd63a0>
True
[[36m2022-06-15 09:39:40,017[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_ELU/2022-06-15_09-35-11/experiment=gauss_03/checkpoints/epoch_002.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:39:41,904[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:39:41,904[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_04[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:39:43,168[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:39:43,169[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       net:                                                                    
â”‚         _target_: src.models.components.gauss_net.SimpleDenseNet_SS_gauss     
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss                                                      
â”‚         name: Dense_3_Hidden_128_64_32_SoftShrink                             
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - Dense_3_Hidden_128_64_32_SoftShrink                                 
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ Dense_3_Hidden_128_64_32_SoftShrink                                     
[[36m2022-06-15 09:39:43,197[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:39:43,199[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:39:43,402[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpmlro1m_n[0m
[[36m2022-06-15 09:39:43,402[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpmlro1m_n/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:39:43,409[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:39:43,410[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:39:43,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:39:43,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:39:43,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_093944-36m2alcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_SoftShrink
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss/runs/36m2alcn
[[36m2022-06-15 09:39:48,231[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:39:48,259[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:39:48,259[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:39:48,262[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:39:50,358[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type                    â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ SimpleDenseNet_SS_gauss â”‚ 23.8 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential              â”‚ 23.8 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear                  â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d             â”‚    256 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Softshrink              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear                  â”‚  8.3 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d             â”‚    128 â”‚
â”‚ 7  â”‚ net.model.5  â”‚ Softshrink              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear                  â”‚  2.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d             â”‚     64 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Softshrink              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear                  â”‚     66 â”‚
â”‚ 12 â”‚ criterion    â”‚ MSELoss                 â”‚      0 â”‚
â”‚ 13 â”‚ train_acc    â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 14 â”‚ val_acc      â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 15 â”‚ test_acc     â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 16 â”‚ val_acc_best â”‚ MinMetric               â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        281.72it/s loss: 0.217      
                                   0:00:00                     v_num: alcn      
                                                               val/acc: 0.147   
                                                               val/acc_best:    
                                                               0.122 train/acc: 
                                                               0.242            
[[36m2022-06-15 09:44:01,630[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:44:01,632[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_SoftShrink/2022-06-15_09-39-41/experiment=gauss_04/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-15 09:44:01,636[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:44:01,637[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_SoftShrink/2022-06-15_09-39-41/experiment=gauss_04/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    1.9462816715240479     â”‚
â”‚         test/loss         â”‚    1.9462816715240479     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 778.22it/s 
[[36m2022-06-15 09:44:02,112[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.362 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.362 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 1.94628
wandb:           test/loss 1.94628
wandb:           train/acc 0.24216
wandb:          train/loss 0.24216
wandb: trainer/global_step 62500
wandb:             val/acc 0.14683
wandb:        val/acc_best 0.12181
wandb:            val/loss 0.14683
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_SoftShrink: https://wandb.ai/rshimogawa/nngauss/runs/36m2alcn
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_093944-36m2alcn/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f642504c310>
True
[[36m2022-06-15 09:44:06,358[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_SoftShrink/2022-06-15_09-39-41/experiment=gauss_04/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:44:08,386[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:44:08,387[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_05[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:44:09,705[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:44:09,705[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       net:                                                                    
â”‚         _target_: src.models.components.gauss_net.SimpleDenseNet_LR_gauss     
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss                                                      
â”‚         name: Dense_3_Hidden_128_64_32_LeakyReLU                              
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - Dense_3_Hidden_128_64_32_LeakyReLU                                  
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ Dense_3_Hidden_128_64_32_LeakyReLU                                      
[[36m2022-06-15 09:44:09,733[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:44:09,735[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:44:09,932[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpx528yzns[0m
[[36m2022-06-15 09:44:09,933[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpx528yzns/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:44:09,940[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:44:09,941[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:44:09,941[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:44:09,942[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:44:09,942[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_094410-16sdi56t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_LeakyReLU
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss/runs/16sdi56t
[[36m2022-06-15 09:44:14,636[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:44:14,666[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:44:14,667[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:44:14,667[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:44:14,670[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:44:16,757[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type                    â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ SimpleDenseNet_LR_gauss â”‚ 23.8 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential              â”‚ 23.8 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear                  â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d             â”‚    256 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ LeakyReLU               â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear                  â”‚  8.3 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d             â”‚    128 â”‚
â”‚ 7  â”‚ net.model.5  â”‚ LeakyReLU               â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear                  â”‚  2.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d             â”‚     64 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ LeakyReLU               â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear                  â”‚     66 â”‚
â”‚ 12 â”‚ criterion    â”‚ MSELoss                 â”‚      0 â”‚
â”‚ 13 â”‚ train_acc    â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 14 â”‚ val_acc      â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 15 â”‚ test_acc     â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 16 â”‚ val_acc_best â”‚ MinMetric               â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        281.87it/s loss: 0.25 v_num:
                                   0:00:00                     i56t val/acc:    
                                                               0.124            
                                                               val/acc_best:    
                                                               0.112 train/acc: 
                                                               0.257            
[[36m2022-06-15 09:48:29,372[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:48:29,374[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_LeakyReLU/2022-06-15_09-44-08/experiment=gauss_05/checkpoints/epoch_001.ckpt[0m
[[36m2022-06-15 09:48:29,378[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:48:29,379[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_LeakyReLU/2022-06-15_09-44-08/experiment=gauss_05/checkpoints/epoch_001.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.2891356945037842     â”‚
â”‚         test/loss         â”‚    0.2891356945037842     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 760.21it/s 
[[36m2022-06-15 09:48:29,777[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–†â–„â–…â–…â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–â–ƒâ–ƒâ–…â–ƒâ–„â–ƒâ–‚â–‚â–„â–‚â–ƒâ–ƒâ–ƒâ–â–„â–ƒâ–‚â–‚â–„â–…â–‚â–„â–ƒâ–„â–‚â–„â–‚
wandb:        val/acc_best â–ˆâ–†â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–†â–„â–…â–…â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–â–ƒâ–ƒâ–…â–ƒâ–„â–ƒâ–‚â–‚â–„â–‚â–ƒâ–ƒâ–ƒâ–â–„â–ƒâ–‚â–‚â–„â–…â–‚â–„â–ƒâ–„â–‚â–„â–‚
wandb: 
wandb: Run summary:
wandb:               epoch 1
wandb:            test/acc 0.28914
wandb:           test/loss 0.28914
wandb:           train/acc 0.25665
wandb:          train/loss 0.25665
wandb: trainer/global_step 62500
wandb:             val/acc 0.12449
wandb:        val/acc_best 0.11161
wandb:            val/loss 0.12449
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_LeakyReLU: https://wandb.ai/rshimogawa/nngauss/runs/16sdi56t
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_094410-16sdi56t/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f064be0ae80>
True
[[36m2022-06-15 09:48:34,041[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_LeakyReLU/2022-06-15_09-44-08/experiment=gauss_05/checkpoints/epoch_001.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:48:35,988[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:48:35,988[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_06[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:48:37,284[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:48:37,285[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       net:                                                                    
â”‚         _target_: src.models.components.gauss_net.SimpleDenseNet_LL_gauss     
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss                                                      
â”‚         name: Dense_3_Hidden_128_64_32_Linear                                 
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - Dense_3_Hidden_128_64_32_Linear                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ Dense_3_Hidden_128_64_32_Linear                                         
[[36m2022-06-15 09:48:37,313[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:48:37,316[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-15 09:48:37,494[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpwuxkl9x8[0m
[[36m2022-06-15 09:48:37,494[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpwuxkl9x8/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 09:48:37,500[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 09:48:37,501[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 09:48:37,502[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 09:48:37,502[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 09:48:37,503[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_094838-3i5ed9q4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_Linear
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss/runs/3i5ed9q4
[[36m2022-06-15 09:48:42,394[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 09:48:42,422[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 09:48:42,423[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 09:48:42,423[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 09:48:42,426[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 09:48:44,525[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type                    â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ SimpleDenseNet_LL_gauss â”‚ 23.8 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential              â”‚ 23.8 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear                  â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d             â”‚    256 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear                  â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ BatchNorm1d             â”‚    128 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear                  â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ BatchNorm1d             â”‚     64 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear                  â”‚     66 â”‚
â”‚ 9  â”‚ criterion    â”‚ MSELoss                 â”‚      0 â”‚
â”‚ 10 â”‚ train_acc    â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 11 â”‚ val_acc      â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 12 â”‚ test_acc     â”‚ MeanSquaredError        â”‚      0 â”‚
â”‚ 13 â”‚ val_acc_best â”‚ MinMetric               â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.8 K                                                        
Non-trainable params: 0                                                         
Total params: 23.8 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        287.33it/s loss: 4.41 v_num:
                                   0:00:00                     d9q4 val/acc:    
                                                               4.251            
                                                               val/acc_best:    
                                                               4.201 train/acc: 
                                                               4.514            
[[36m2022-06-15 09:52:52,827[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 09:52:52,829[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_Linear/2022-06-15_09-48-35/experiment=gauss_06/checkpoints/epoch_077.ckpt[0m
[[36m2022-06-15 09:52:52,833[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 09:52:52,833[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_Linear/2022-06-15_09-48-35/experiment=gauss_06/checkpoints/epoch_077.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    4.5567851066589355     â”‚
â”‚         test/loss         â”‚    4.5567851066589355     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 769.75it/s 
[[36m2022-06-15 09:52:53,279[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: - 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: \ 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: | 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb: / 0.405 MB of 0.405 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–…â–„â–â–‚â–ƒâ–‚â–‡â–ƒâ–â–â–‚â–‚â–â–‚â–ƒâ–â–‚â–„â–ƒâ–‚â–ƒâ–‚â–‚â–„â–‚â–ƒâ–‚â–…â–ˆâ–‚â–â–…â–ƒ
wandb:        val/acc_best â–ˆâ–…â–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–…â–„â–â–‚â–ƒâ–‚â–‡â–ƒâ–â–â–‚â–‚â–â–‚â–ƒâ–â–‚â–„â–ƒâ–‚â–ƒâ–‚â–‚â–„â–‚â–ƒâ–‚â–…â–ˆâ–‚â–â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:               epoch 77
wandb:            test/acc 4.55679
wandb:           test/loss 4.55679
wandb:           train/acc 4.51439
wandb:          train/loss 4.51439
wandb: trainer/global_step 62500
wandb:             val/acc 4.25132
wandb:        val/acc_best 4.20076
wandb:            val/loss 4.25132
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_Linear: https://wandb.ai/rshimogawa/nngauss/runs/3i5ed9q4
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_094838-3i5ed9q4/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4fead456a0>
True
[[36m2022-06-15 09:52:57,918[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_Linear/2022-06-15_09-48-35/experiment=gauss_06/checkpoints/epoch_077.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 09:52:59,898[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 09:52:59,898[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_07[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 09:53:01,366[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 09:53:01,366[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       net:                                                                    
â”‚         _target_: src.models.components.gauss_net.SimpleDenseNet_CNN_gauss    
â”‚         input_size: 100                                                       
â”‚         lin1_size: 64                                                         
â”‚         lin2_size: 32                                                         
â”‚         lin3_size: 16                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss                                                      
â”‚         name: CNN                                                             
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - CNN                                                                 
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ CNN                                                                     
[[36m2022-06-15 09:53:01,395[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 09:53:01,397[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
Error executing job with overrides: ['experiment=gauss_07']
Error locating target 'src.models.components.gauss_net.SimpleDenseNet_CNN_gauss', see chained exception above.
full_key: model.net

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 10:29:00,296[0m][[35mHYDRA[0m] Launching 0 jobs locally[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-15 11:37:16,705[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-15 11:37:16,705[0m][[35mHYDRA[0m] 	#0 : experiment=gauss_11_LR_MAE[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-15 11:37:17,789[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-15 11:37:17,790[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module_mae.gaussLitModule                    
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       net:                                                                    
â”‚         _target_: src.models.components.gauss_net.CNN2_gauss                  
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss                                                      
â”‚         name: Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE                    
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE                        
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE                            
[[36m2022-06-15 11:37:17,819[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-15 11:37:17,822[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module_mae.gaussLitModule>[0m
[[36m2022-06-15 11:37:18,048[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp0rfgyj6v[0m
[[36m2022-06-15 11:37:18,048[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp0rfgyj6v/_remote_module_non_sriptable.py[0m
[[36m2022-06-15 11:37:18,055[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-15 11:37:18,056[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-15 11:37:18,057[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-15 11:37:18,057[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-15 11:37:18,058[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220615_113718-2yrhbtdg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss/runs/2yrhbtdg
[[36m2022-06-15 11:37:22,792[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-15 11:37:22,819[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-15 11:37:22,819[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-15 11:37:22,822[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-15 11:37:24,901[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ CNN2_gauss        â”‚ 26.0 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 26.0 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Unflatten         â”‚      0 â”‚
â”‚ 3  â”‚ net.model.1  â”‚ Conv1d            â”‚      8 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Flatten           â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚ 12.9 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚    256 â”‚
â”‚ 7  â”‚ net.model.5  â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  8.3 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚  1.1 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  1.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚     66 â”‚
â”‚ 21 â”‚ criterion    â”‚ L1Loss            â”‚      0 â”‚
â”‚ 22 â”‚ train_acc    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_acc      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_acc     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ val_acc_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 26.0 K                                                        
Non-trainable params: 0                                                         
Total params: 26.0 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 149 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:03 â€¢        216.90it/s loss: 0.303      
                                   0:00:00                     v_num: btdg      
                                                               val/acc: 0.181   
                                                               val/acc_best:    
                                                               0.151 train/acc: 
                                                               0.274            
[[36m2022-06-15 11:45:33,787[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-15 11:45:33,790[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE/2022-06-15_11-37-16/experiment=gauss_11_LR_MAE/checkpoints/epoch_049.ckpt[0m
[[36m2022-06-15 11:45:33,796[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-15 11:45:33,796[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE/2022-06-15_11-37-16/experiment=gauss_11_LR_MAE/checkpoints/epoch_049.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚      0.3792724609375      â”‚
â”‚         test/loss         â”‚      0.3792724609375      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 674.28it/s 
[[36m2022-06-15 11:45:34,206[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.362 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–†â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–†â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             val/acc â–…â–…â–„â–…â–„â–‚â–‚â–…â–ƒâ–‡â–ƒâ–„â–ƒâ–ˆâ–…â–ƒâ–…â–‚â–‚â–ƒâ–‚â–„â–„â–„â–ƒâ–„â–…â–â–‚â–†â–‚â–„â–…â–ƒâ–‡â–â–ƒâ–†â–ƒâ–‚
wandb:        val/acc_best â–ˆâ–ˆâ–…â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–…â–…â–„â–…â–„â–‚â–‚â–…â–ƒâ–‡â–ƒâ–„â–ƒâ–ˆâ–…â–ƒâ–…â–‚â–‚â–ƒâ–‚â–„â–„â–„â–ƒâ–„â–…â–â–‚â–†â–‚â–„â–…â–ƒâ–‡â–â–ƒâ–†â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:               epoch 49
wandb:            test/acc 0.37927
wandb:           test/loss 0.37927
wandb:           train/acc 0.27363
wandb:          train/loss 0.27363
wandb: trainer/global_step 93750
wandb:             val/acc 0.18145
wandb:        val/acc_best 0.15089
wandb:            val/loss 0.18145
wandb: 
wandb: Synced Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE: https://wandb.ai/rshimogawa/nngauss/runs/2yrhbtdg
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220615_113718-2yrhbtdg/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f45a1847550>
True
[[36m2022-06-15 11:45:38,300[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/Dense_3_Hidden_128_64_32_32_32_LeakyReLU_MAE/2022-06-15_11-37-16/experiment=gauss_11_LR_MAE/checkpoints/epoch_049.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:23:51,496[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:23:51,496[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:23:52,510[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:23:52,510[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H3_ReLU_128_64_32                                                 
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H3_ReLU                        
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H3_ReLU_128_64_32                                               
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H3_ReLU_128_64_32                                                   
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H3_ReLU_128_64_32                                                       
[[36m2022-06-16 12:23:52,539[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:23:52,542[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:23:52,736[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmplo09_ea2[0m
[[36m2022-06-16 12:23:52,736[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmplo09_ea2/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:23:52,742[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:23:52,743[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:23:52,744[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:23:52,744[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:23:52,744[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_122353-6psq7it0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_ReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/6psq7it0
[[36m2022-06-16 12:23:58,545[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:23:58,576[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:23:58,577[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:23:58,577[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:23:58,583[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:24:01,228[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H3_ReLU          â”‚ 23.3 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 23.3 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ReLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ReLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚     66 â”‚
â”‚ 9  â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 10 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 11 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 12 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        334.18it/s loss: 0.0187     
                                   0:00:00                     v_num: 7it0      
                                                               val/acc: 0.019   
                                                               val/acc_best:    
                                                               0.019 train/acc: 
                                                               0.021            
[[36m2022-06-16 12:27:29,852[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:27:30,456[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ReLU_128_64_32/2022-06-16_12-23-51/experiment=NN/H3_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:27:30,459[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:27:30,460[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ReLU_128_64_32/2022-06-16_12-23-51/experiment=NN/H3_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.21474191546440125    â”‚
â”‚         test/loss         â”‚    0.21474191546440125    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 842.50it/s 
[[36m2022-06-16 12:27:30,925[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.393 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.398 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.21474
wandb:           test/loss 0.21474
wandb:           train/acc 0.02059
wandb:          train/loss 0.02059
wandb: trainer/global_step 62500
wandb:             val/acc 0.0189
wandb:        val/acc_best 0.0189
wandb:            val/loss 0.0189
wandb: 
wandb: Synced H3_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/6psq7it0
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_122353-6psq7it0/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fb533154250>
True
[[36m2022-06-16 12:27:35,640[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ReLU_128_64_32/2022-06-16_12-23-51/experiment=NN/H3_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:27:37,625[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:27:37,625[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:27:38,676[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:27:38,677[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H4_ReLU_128_64_32                                                 
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H4_ReLU                        
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H4_ReLU_128_64_32                                               
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H4_ReLU_128_64_32                                                   
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H4_ReLU_128_64_32                                                       
[[36m2022-06-16 12:27:38,706[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:27:38,709[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:27:38,903[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp78u34q5k[0m
[[36m2022-06-16 12:27:38,904[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp78u34q5k/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:27:38,910[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:27:38,911[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:27:38,912[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:27:38,912[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:27:38,913[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_122739-1qvh86ai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_ReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1qvh86ai
[[36m2022-06-16 12:27:43,800[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:27:43,830[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:27:43,831[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:27:43,831[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:27:43,835[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:27:46,459[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H4_ReLU          â”‚ 24.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 24.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ReLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ReLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ReLU             â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚     66 â”‚
â”‚ 11 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 12 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 14 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        315.16it/s loss: 0.0254     
                                   0:00:00                     v_num: 86ai      
                                                               val/acc: 0.016   
                                                               val/acc_best:    
                                                               0.016 train/acc: 
                                                               0.016            
[[36m2022-06-16 12:31:30,919[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:31:31,446[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ReLU_128_64_32/2022-06-16_12-27-37/experiment=NN/H4_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:31:31,450[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:31:31,450[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ReLU_128_64_32/2022-06-16_12-27-37/experiment=NN/H4_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.21019525825977325    â”‚
â”‚         test/loss         â”‚    0.21019525825977325    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 816.40it/s 
[[36m2022-06-16 12:31:31,914[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.169 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.210 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.210 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.210 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.309 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.340 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.340 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.2102
wandb:           test/loss 0.2102
wandb:           train/acc 0.01604
wandb:          train/loss 0.01604
wandb: trainer/global_step 62500
wandb:             val/acc 0.01619
wandb:        val/acc_best 0.01595
wandb:            val/loss 0.01619
wandb: 
wandb: Synced H4_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1qvh86ai
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_122739-1qvh86ai/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7efea5caa1f0>
True
[[36m2022-06-16 12:31:41,719[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ReLU_128_64_32/2022-06-16_12-27-37/experiment=NN/H4_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:31:43,710[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:31:43,710[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:31:44,768[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:31:44,768[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H5_ReLU_128_64_32                                                 
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H5_ReLU                        
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H5_ReLU_128_64_32                                               
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H5_ReLU_128_64_32                                                   
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H5_ReLU_128_64_32                                                       
[[36m2022-06-16 12:31:44,798[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:31:44,800[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:31:44,982[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpxnu6nrqf[0m
[[36m2022-06-16 12:31:44,985[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpxnu6nrqf/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:31:44,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:31:44,992[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:31:44,993[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:31:44,993[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:31:44,994[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_123145-mfnz774o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_ReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/mfnz774o
[[36m2022-06-16 12:31:49,693[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:31:49,723[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:31:49,723[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:31:49,727[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:31:52,574[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H5_ReLU          â”‚ 25.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 25.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ReLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ReLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ReLU             â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ ReLU             â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚     66 â”‚
â”‚ 13 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 14 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 16 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 17 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        292.62it/s loss: 0.0154     
                                   0:00:00                     v_num: 774o      
                                                               val/acc: 0.013   
                                                               val/acc_best:    
                                                               0.012 train/acc: 
                                                               0.013            
[[36m2022-06-16 12:35:51,894[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:35:52,448[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ReLU_128_64_32/2022-06-16_12-31-43/experiment=NN/H5_ReLU.yaml/checkpoints/epoch_002.ckpt[0m
[[36m2022-06-16 12:35:52,451[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:35:52,452[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ReLU_128_64_32/2022-06-16_12-31-43/experiment=NN/H5_ReLU.yaml/checkpoints/epoch_002.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.2580607533454895     â”‚
â”‚         test/loss         â”‚    0.2580607533454895     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 799.36it/s 
[[36m2022-06-16 12:35:52,929[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.399 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.404 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 2
wandb:            test/acc 0.25806
wandb:           test/loss 0.25806
wandb:           train/acc 0.01314
wandb:          train/loss 0.01314
wandb: trainer/global_step 62500
wandb:             val/acc 0.01329
wandb:        val/acc_best 0.01223
wandb:            val/loss 0.01329
wandb: 
wandb: Synced H5_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/mfnz774o
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_123145-mfnz774o/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f8d00a5a370>
True
[[36m2022-06-16 12:35:57,710[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ReLU_128_64_32/2022-06-16_12-31-43/experiment=NN/H5_ReLU.yaml/checkpoints/epoch_002.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:35:59,696[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:35:59,696[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H10_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:36:00,762[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:36:00,762[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H10_ReLU_128_64_32                                                
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H10_ReLU                       
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H10_ReLU_128_64_32                                              
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H10_ReLU_128_64_32                                                  
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H10_ReLU_128_64_32                                                      
[[36m2022-06-16 12:36:00,792[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:36:00,795[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:36:00,983[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpwhbr0o6r[0m
[[36m2022-06-16 12:36:00,983[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpwhbr0o6r/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:36:00,990[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:36:00,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:36:00,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:36:00,991[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:36:00,992[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_123601-2ax59uyk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H10_ReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/2ax59uyk
[[36m2022-06-16 12:36:06,735[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:36:06,767[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:36:06,767[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:36:06,772[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:36:09,414[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H10_ReLU         â”‚ 30.7 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 30.7 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ReLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ReLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ReLU             â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ ReLU             â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU             â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ ReLU             â”‚      0 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 17 â”‚ net.model.15 â”‚ ReLU             â”‚      0 â”‚
â”‚ 18 â”‚ net.model.16 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU             â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 21 â”‚ net.model.19 â”‚ ReLU             â”‚      0 â”‚
â”‚ 22 â”‚ net.model.20 â”‚ Linear           â”‚     66 â”‚
â”‚ 23 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 24 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 25 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 26 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 27 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 30.7 K                                                        
Non-trainable params: 0                                                         
Total params: 30.7 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:03 â€¢        223.28it/s loss: 0.022      
                                   0:00:00                     v_num: 9uyk      
                                                               val/acc: 0.025   
                                                               val/acc_best:    
                                                               0.018 train/acc: 
                                                               0.024            
[[36m2022-06-16 12:41:20,167[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:41:20,768[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H10_ReLU_128_64_32/2022-06-16_12-35-59/experiment=NN/H10_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:41:20,774[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:41:20,775[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H10_ReLU_128_64_32/2022-06-16_12-35-59/experiment=NN/H10_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.5824992060661316     â”‚
â”‚         test/loss         â”‚    0.5824992060661316     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 733.59it/s 
[[36m2022-06-16 12:41:21,255[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.395 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.397 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.397 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.403 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.5825
wandb:           test/loss 0.5825
wandb:           train/acc 0.02418
wandb:          train/loss 0.02418
wandb: trainer/global_step 62500
wandb:             val/acc 0.02525
wandb:        val/acc_best 0.0177
wandb:            val/loss 0.02525
wandb: 
wandb: Synced H10_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/2ax59uyk
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_123601-2ax59uyk/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f97f97ce460>
True
[[36m2022-06-16 12:41:26,742[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H10_ReLU_128_64_32/2022-06-16_12-35-59/experiment=NN/H10_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:41:28,734[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:41:28,734[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H15_ReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:41:29,792[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:41:29,792[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H15_ReLU_128_64_32                                                
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H15_ReLU                       
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H15_ReLU_128_64_32                                              
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H15_ReLU_128_64_32                                                  
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H15_ReLU_128_64_32                                                      
[[36m2022-06-16 12:41:29,826[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:41:29,829[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:41:30,015[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpui62oj6v[0m
[[36m2022-06-16 12:41:30,015[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpui62oj6v/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:41:30,022[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:41:30,023[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:41:30,024[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:41:30,024[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:41:30,024[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_124130-4of2l252
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H15_ReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/4of2l252
[[36m2022-06-16 12:41:36,488[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:41:36,514[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:41:36,515[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:41:36,515[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:41:36,519[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:41:39,273[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H15_ReLU         â”‚ 36.0 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 36.0 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ReLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ReLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ReLU             â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ ReLU             â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU             â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ ReLU             â”‚      0 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 17 â”‚ net.model.15 â”‚ ReLU             â”‚      0 â”‚
â”‚ 18 â”‚ net.model.16 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU             â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 21 â”‚ net.model.19 â”‚ ReLU             â”‚      0 â”‚
â”‚ 22 â”‚ net.model.20 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 23 â”‚ net.model.21 â”‚ ReLU             â”‚      0 â”‚
â”‚ 24 â”‚ net.model.22 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 25 â”‚ net.model.23 â”‚ ReLU             â”‚      0 â”‚
â”‚ 26 â”‚ net.model.24 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 27 â”‚ net.model.25 â”‚ ReLU             â”‚      0 â”‚
â”‚ 28 â”‚ net.model.26 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 29 â”‚ net.model.27 â”‚ ReLU             â”‚      0 â”‚
â”‚ 30 â”‚ net.model.28 â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 31 â”‚ net.model.29 â”‚ ReLU             â”‚      0 â”‚
â”‚ 32 â”‚ net.model.30 â”‚ Linear           â”‚     66 â”‚
â”‚ 33 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 34 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 35 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 36 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 37 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 36.0 K                                                        
Non-trainable params: 0                                                         
Total params: 36.0 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:03 â€¢        183.58it/s loss: 0.0489     
                                   0:00:00                     v_num: l252      
                                                               val/acc: 0.027   
                                                               val/acc_best:    
                                                               0.019 train/acc: 
                                                               0.029            
[[36m2022-06-16 12:48:06,785[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:48:07,498[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H15_ReLU_128_64_32/2022-06-16_12-41-28/experiment=NN/H15_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:48:07,505[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:48:07,506[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H15_ReLU_128_64_32/2022-06-16_12-41-28/experiment=NN/H15_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚     1.150724172592163     â”‚
â”‚         test/loss         â”‚     1.150724172592163     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 662.78it/s 
[[36m2022-06-16 12:48:08,067[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.389 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.396 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.389 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 1.15072
wandb:           test/loss 1.15072
wandb:           train/acc 0.02944
wandb:          train/loss 0.02944
wandb: trainer/global_step 62500
wandb:             val/acc 0.02747
wandb:        val/acc_best 0.01872
wandb:            val/loss 0.02747
wandb: 
wandb: Synced H15_ReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/4of2l252
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_124130-4of2l252/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fcd759051c0>
True
[[36m2022-06-16 12:48:13,348[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H15_ReLU_128_64_32/2022-06-16_12-41-28/experiment=NN/H15_ReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:48:15,341[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:48:15,341[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_SiLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:48:16,413[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:48:16,413[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H3_SiLU_128_64_32                                                 
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H3_SiLU                        
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H3_SiLU_128_64_32                                               
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H3_SiLU_128_64_32                                                   
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H3_SiLU_128_64_32                                                       
[[36m2022-06-16 12:48:16,442[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:48:16,445[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:48:16,625[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpp1oinyaz[0m
[[36m2022-06-16 12:48:16,626[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpp1oinyaz/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:48:16,632[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:48:16,633[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:48:16,633[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:48:16,634[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:48:16,634[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_124817-2ql9x1k6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_SiLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/2ql9x1k6
[[36m2022-06-16 12:48:21,377[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:48:21,406[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:48:21,407[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:48:21,407[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:48:21,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:48:24,110[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H3_SiLU          â”‚ 23.3 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 23.3 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ SiLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ SiLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ SiLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚     66 â”‚
â”‚ 9  â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 10 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 11 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 12 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        338.07it/s loss: 0.0409     
                                   0:00:00                     v_num: x1k6      
                                                               val/acc: 0.031   
                                                               val/acc_best:    
                                                               0.026 train/acc: 
                                                               0.033            
[[36m2022-06-16 12:51:52,165[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:51:52,802[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_SiLU_128_64_32/2022-06-16_12-48-15/experiment=NN/H3_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:51:52,805[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:51:52,805[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_SiLU_128_64_32/2022-06-16_12-48-15/experiment=NN/H3_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.1421467661857605     â”‚
â”‚         test/loss         â”‚    0.1421467661857605     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 838.01it/s 
[[36m2022-06-16 12:51:53,273[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.14215
wandb:           test/loss 0.14215
wandb:           train/acc 0.03286
wandb:          train/loss 0.03286
wandb: trainer/global_step 62500
wandb:             val/acc 0.03125
wandb:        val/acc_best 0.02564
wandb:            val/loss 0.03125
wandb: 
wandb: Synced H3_SiLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/2ql9x1k6
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_124817-2ql9x1k6/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7eff11ac5f70>
True
[[36m2022-06-16 12:51:58,284[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_SiLU_128_64_32/2022-06-16_12-48-15/experiment=NN/H3_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:52:00,272[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:52:00,272[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_SiLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:52:01,328[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:52:01,328[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H4_SiLU_128_64_32                                                 
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H4_SiLU                        
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H4_SiLU_128_64_32                                               
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H4_SiLU_128_64_32                                                   
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H4_SiLU_128_64_32                                                       
[[36m2022-06-16 12:52:01,358[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:52:01,360[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:52:01,550[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpv72kakih[0m
[[36m2022-06-16 12:52:01,550[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpv72kakih/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:52:01,557[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:52:01,558[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:52:01,558[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:52:01,559[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:52:01,559[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_125202-1k9qjw75
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_SiLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1k9qjw75
[[36m2022-06-16 12:52:09,673[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:52:09,704[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:52:09,705[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:52:09,711[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:52:12,363[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H4_SiLU          â”‚ 24.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 24.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ SiLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ SiLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ SiLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ SiLU             â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚     66 â”‚
â”‚ 11 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 12 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 14 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        316.59it/s loss: 0.0285     
                                   0:00:00                     v_num: jw75      
                                                               val/acc: 0.024   
                                                               val/acc_best:    
                                                               0.023 train/acc: 
                                                               0.025            
[[36m2022-06-16 12:55:56,743[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 12:55:57,273[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_SiLU_128_64_32/2022-06-16_12-52-00/experiment=NN/H4_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 12:55:57,276[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 12:55:57,276[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_SiLU_128_64_32/2022-06-16_12-52-00/experiment=NN/H4_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.1403432935476303     â”‚
â”‚         test/loss         â”‚    0.1403432935476303     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 821.29it/s 
[[36m2022-06-16 12:55:57,747[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.198 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.198 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.198 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.202 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.207 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.215 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.387 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.14034
wandb:           test/loss 0.14034
wandb:           train/acc 0.02529
wandb:          train/loss 0.02529
wandb: trainer/global_step 62500
wandb:             val/acc 0.02368
wandb:        val/acc_best 0.02278
wandb:            val/loss 0.02368
wandb: 
wandb: Synced H4_SiLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1k9qjw75
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_125202-1k9qjw75/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f25e14371c0>
True
[[36m2022-06-16 12:56:02,395[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_SiLU_128_64_32/2022-06-16_12-52-00/experiment=NN/H4_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 12:56:04,386[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 12:56:04,386[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_SiLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 12:56:05,472[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 12:56:05,472[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H5_SiLU_128_64_32                                                 
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H5_SiLU                        
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H5_SiLU_128_64_32                                               
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H5_SiLU_128_64_32                                                   
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H5_SiLU_128_64_32                                                       
[[36m2022-06-16 12:56:05,502[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 12:56:05,505[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 12:56:05,700[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmppp2m_bzq[0m
[[36m2022-06-16 12:56:05,700[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmppp2m_bzq/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 12:56:05,706[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 12:56:05,707[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 12:56:05,708[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 12:56:05,708[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 12:56:05,709[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_125606-hifbh0jw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_SiLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/hifbh0jw
[[36m2022-06-16 12:56:10,500[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 12:56:10,526[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 12:56:10,527[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 12:56:10,527[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 12:56:10,531[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 12:56:13,184[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H5_SiLU          â”‚ 25.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 25.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ SiLU             â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ SiLU             â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ SiLU             â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ SiLU             â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ SiLU             â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚     66 â”‚
â”‚ 13 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 14 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 16 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 17 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        300.20it/s loss: 0.0323     
                                   0:00:00                     v_num: h0jw      
                                                               val/acc: 0.033   
                                                               val/acc_best:    
                                                               0.028 train/acc: 
                                                               0.033            
[[36m2022-06-16 13:00:12,781[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:00:13,384[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_SiLU_128_64_32/2022-06-16_12-56-04/experiment=NN/H5_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:00:13,387[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:00:13,388[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_SiLU_128_64_32/2022-06-16_12-56-04/experiment=NN/H5_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.11632352322340012    â”‚
â”‚         test/loss         â”‚    0.11632352322340012    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 826.02it/s 
[[36m2022-06-16 13:00:13,845[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.396 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.401 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.402 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–†â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–
wandb:        val/acc_best â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–†â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.11632
wandb:           test/loss 0.11632
wandb:           train/acc 0.03269
wandb:          train/loss 0.03269
wandb: trainer/global_step 62500
wandb:             val/acc 0.0326
wandb:        val/acc_best 0.02838
wandb:            val/loss 0.0326
wandb: 
wandb: Synced H5_SiLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/hifbh0jw
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_125606-hifbh0jw/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f622eb4f250>
True
[[36m2022-06-16 13:00:17,872[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_SiLU_128_64_32/2022-06-16_12-56-04/experiment=NN/H5_SiLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:00:19,863[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:00:19,863[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_LeakyReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:00:20,909[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:00:20,909[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H3_LeakyReLU_128_64_32                                            
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H3_LeakyReLU                   
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H3_LeakyReLU_128_64_32                                          
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H3_LeakyReLU_128_64_32                                              
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H3_LeakyReLU_128_64_32                                                  
[[36m2022-06-16 13:00:20,938[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:00:20,941[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:00:21,130[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpy7c2jitq[0m
[[36m2022-06-16 13:00:21,131[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpy7c2jitq/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:00:21,137[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:00:21,138[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:00:21,138[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:00:21,139[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:00:21,139[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_130022-3ld4thqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_LeakyReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/3ld4thqq
[[36m2022-06-16 13:00:26,030[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:00:26,058[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:00:26,059[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:00:26,059[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:00:26,063[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:00:28,704[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H3_LeakyReLU     â”‚ 23.3 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 23.3 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚     66 â”‚
â”‚ 9  â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 10 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 11 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 12 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        341.63it/s loss: 0.0178     
                                   0:00:00                     v_num: thqq      
                                                               val/acc: 0.025   
                                                               val/acc_best:    
                                                               0.021 train/acc: 
                                                               0.022            
[[36m2022-06-16 13:03:58,885[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:03:59,536[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_LeakyReLU_128_64_32/2022-06-16_13-00-19/experiment=NN/H3_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:03:59,539[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:03:59,540[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_LeakyReLU_128_64_32/2022-06-16_13-00-19/experiment=NN/H3_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.24686571955680847    â”‚
â”‚         test/loss         â”‚    0.24686571955680847    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 854.26it/s 
[[36m2022-06-16 13:03:59,997[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.24687
wandb:           test/loss 0.24687
wandb:           train/acc 0.0222
wandb:          train/loss 0.0222
wandb: trainer/global_step 62500
wandb:             val/acc 0.02464
wandb:        val/acc_best 0.02065
wandb:            val/loss 0.02464
wandb: 
wandb: Synced H3_LeakyReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/3ld4thqq
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_130022-3ld4thqq/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f96d59bab20>
True
[[36m2022-06-16 13:04:06,411[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_LeakyReLU_128_64_32/2022-06-16_13-00-19/experiment=NN/H3_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:04:08,395[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:04:08,395[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_LeakyReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:04:09,468[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:04:09,469[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H4_LeakyReLU_128_64_32                                            
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H4_LeakyReLU                   
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H4_LeakyReLU_128_64_32                                          
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H4_LeakyReLU_128_64_32                                              
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H4_LeakyReLU_128_64_32                                                  
[[36m2022-06-16 13:04:09,498[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:04:09,500[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:04:09,684[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp99rfpttd[0m
[[36m2022-06-16 13:04:09,686[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp99rfpttd/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:04:09,693[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:04:09,694[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:04:09,694[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:04:09,695[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:04:09,695[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_130410-1lk2sjfz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_LeakyReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1lk2sjfz
[[36m2022-06-16 13:04:15,392[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:04:15,418[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:04:15,419[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:04:15,419[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:04:15,423[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:04:18,098[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H4_LeakyReLU     â”‚ 24.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 24.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚     66 â”‚
â”‚ 11 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 12 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 14 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        313.06it/s loss: 0.0244     
                                   0:00:00                     v_num: sjfz      
                                                               val/acc: 0.017   
                                                               val/acc_best:    
                                                               0.017 train/acc: 
                                                               0.018            
[[36m2022-06-16 13:08:01,888[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:08:02,498[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_LeakyReLU_128_64_32/2022-06-16_13-04-08/experiment=NN/H4_LeakyReLU.yaml/checkpoints/epoch_001.ckpt[0m
[[36m2022-06-16 13:08:02,501[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:08:02,502[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_LeakyReLU_128_64_32/2022-06-16_13-04-08/experiment=NN/H4_LeakyReLU.yaml/checkpoints/epoch_001.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.2511289417743683     â”‚
â”‚         test/loss         â”‚    0.2511289417743683     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 828.34it/s 
[[36m2022-06-16 13:08:02,969[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 1
wandb:            test/acc 0.25113
wandb:           test/loss 0.25113
wandb:           train/acc 0.01759
wandb:          train/loss 0.01759
wandb: trainer/global_step 62500
wandb:             val/acc 0.01666
wandb:        val/acc_best 0.01666
wandb:            val/loss 0.01666
wandb: 
wandb: Synced H4_LeakyReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1lk2sjfz
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_130410-1lk2sjfz/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4b431baca0>
True
[[36m2022-06-16 13:08:07,626[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_LeakyReLU_128_64_32/2022-06-16_13-04-08/experiment=NN/H4_LeakyReLU.yaml/checkpoints/epoch_001.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:08:09,606[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:08:09,606[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_LeakyReLU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:08:10,722[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:08:10,722[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H5_LeakyReLU_128_64_32                                            
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H5_LeakyReLU                   
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H5_LeakyReLU_128_64_32                                          
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H5_LeakyReLU_128_64_32                                              
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H5_LeakyReLU_128_64_32                                                  
[[36m2022-06-16 13:08:10,752[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:08:10,755[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:08:10,942[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpa68kinxp[0m
[[36m2022-06-16 13:08:10,943[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpa68kinxp/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:08:10,949[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:08:10,950[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:08:10,950[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:08:10,951[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:08:10,951[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_130811-2ai36gr3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_LeakyReLU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/2ai36gr3
[[36m2022-06-16 13:08:15,654[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:08:15,683[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:08:15,683[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:08:15,687[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:08:18,334[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H5_LeakyReLU     â”‚ 25.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 25.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ LeakyReLU        â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚     66 â”‚
â”‚ 13 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 14 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 16 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 17 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        295.45it/s loss: 0.0134     
                                   0:00:00                     v_num: 6gr3      
                                                               val/acc: 0.018   
                                                               val/acc_best:    
                                                               0.017 train/acc: 
                                                               0.018            
[[36m2022-06-16 13:12:17,558[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:12:18,138[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_LeakyReLU_128_64_32/2022-06-16_13-08-09/experiment=NN/H5_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:12:18,142[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:12:18,143[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_LeakyReLU_128_64_32/2022-06-16_13-08-09/experiment=NN/H5_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.2136443853378296     â”‚
â”‚         test/loss         â”‚    0.2136443853378296     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 815.02it/s 
[[36m2022-06-16 13:12:18,614[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.393 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.21364
wandb:           test/loss 0.21364
wandb:           train/acc 0.0176
wandb:          train/loss 0.0176
wandb: trainer/global_step 62500
wandb:             val/acc 0.0176
wandb:        val/acc_best 0.01713
wandb:            val/loss 0.0176
wandb: 
wandb: Synced H5_LeakyReLU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/2ai36gr3
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_130811-2ai36gr3/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fa8c2e08130>
True
[[36m2022-06-16 13:12:22,538[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_LeakyReLU_128_64_32/2022-06-16_13-08-09/experiment=NN/H5_LeakyReLU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:12:24,522[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:12:24,522[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_ELU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:12:25,576[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:12:25,577[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H3_ELU_128_64_32                                                  
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H3_ELU                         
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H3_ELU_128_64_32                                                
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H3_ELU_128_64_32                                                    
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H3_ELU_128_64_32                                                        
[[36m2022-06-16 13:12:25,606[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:12:25,609[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:12:25,797[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpt_2e_j1g[0m
[[36m2022-06-16 13:12:25,798[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpt_2e_j1g/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:12:25,804[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:12:25,805[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:12:25,805[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:12:25,806[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:12:25,806[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_131226-22c2yeeu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_ELU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/22c2yeeu
[[36m2022-06-16 13:12:30,549[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:12:30,580[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:12:30,581[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:12:30,581[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:12:30,587[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:12:33,189[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H3_ELU           â”‚ 23.3 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 23.3 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ELU              â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ELU              â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ELU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚     66 â”‚
â”‚ 9  â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 10 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 11 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 12 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        335.34it/s loss: 0.0291     
                                   0:00:00                     v_num: yeeu      
                                                               val/acc: 0.025   
                                                               val/acc_best:    
                                                               0.022 train/acc: 
                                                               0.027            
[[36m2022-06-16 13:16:03,336[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:16:03,869[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ELU_128_64_32/2022-06-16_13-12-24/experiment=NN/H3_ELU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:16:03,872[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:16:03,872[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ELU_128_64_32/2022-06-16_13-12-24/experiment=NN/H3_ELU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.11063409596681595    â”‚
â”‚         test/loss         â”‚    0.11063409596681595    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 854.92it/s 
[[36m2022-06-16 13:16:04,333[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.396 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: - 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: \ 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: | 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb: / 0.406 MB of 0.406 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.11063
wandb:           test/loss 0.11063
wandb:           train/acc 0.02672
wandb:          train/loss 0.02672
wandb: trainer/global_step 62500
wandb:             val/acc 0.02538
wandb:        val/acc_best 0.02246
wandb:            val/loss 0.02538
wandb: 
wandb: Synced H3_ELU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/22c2yeeu
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_131226-22c2yeeu/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7ff31dbdcdf0>
True
[[36m2022-06-16 13:16:07,913[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_ELU_128_64_32/2022-06-16_13-12-24/experiment=NN/H3_ELU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:16:09,893[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:16:09,894[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_ELU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:16:10,937[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:16:10,938[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H4_ELU_128_64_32                                                  
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H4_ELU                         
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H4_ELU_128_64_32                                                
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H4_ELU_128_64_32                                                    
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H4_ELU_128_64_32                                                        
[[36m2022-06-16 13:16:10,967[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:16:10,970[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:16:11,156[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp9yjj88t4[0m
[[36m2022-06-16 13:16:11,156[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp9yjj88t4/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:16:11,162[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:16:11,163[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:16:11,163[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:16:11,164[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:16:11,164[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_131612-zu9t9z2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_ELU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/zu9t9z2c
[[36m2022-06-16 13:16:15,916[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:16:15,942[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:16:15,943[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:16:15,943[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:16:15,947[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:16:18,581[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H4_ELU           â”‚ 24.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 24.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ELU              â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ELU              â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ELU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ELU              â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚     66 â”‚
â”‚ 11 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 12 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 14 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        311.73it/s loss: 0.0156     
                                   0:00:00                     v_num: 9z2c      
                                                               val/acc: 0.021   
                                                               val/acc_best:    
                                                               0.02 train/acc:  
                                                               0.022            
[[36m2022-06-16 13:20:00,918[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:20:01,520[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ELU_128_64_32/2022-06-16_13-16-09/experiment=NN/H4_ELU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:20:01,523[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:20:01,524[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ELU_128_64_32/2022-06-16_13-16-09/experiment=NN/H4_ELU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.10119711607694626    â”‚
â”‚         test/loss         â”‚    0.10119711607694626    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 829.62it/s 
[[36m2022-06-16 13:20:01,994[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.395 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.398 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–…â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–…â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.1012
wandb:           test/loss 0.1012
wandb:           train/acc 0.02172
wandb:          train/loss 0.02172
wandb: trainer/global_step 62500
wandb:             val/acc 0.0212
wandb:        val/acc_best 0.02023
wandb:            val/loss 0.0212
wandb: 
wandb: Synced H4_ELU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/zu9t9z2c
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_131612-zu9t9z2c/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f4fbef3d280>
True
[[36m2022-06-16 13:20:06,227[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_ELU_128_64_32/2022-06-16_13-16-09/experiment=NN/H4_ELU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:20:08,207[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:20:08,207[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_ELU.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:20:09,241[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:20:09,242[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H5_ELU_128_64_32                                                  
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H5_ELU                         
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H5_ELU_128_64_32                                                
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H5_ELU_128_64_32                                                    
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H5_ELU_128_64_32                                                        
[[36m2022-06-16 13:20:09,271[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:20:09,274[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:20:09,462[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpkkds2k1f[0m
[[36m2022-06-16 13:20:09,463[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpkkds2k1f/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:20:09,469[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:20:09,470[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:20:09,471[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:20:09,471[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:20:09,471[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_132010-1cl51qao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_ELU_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/1cl51qao
[[36m2022-06-16 13:20:14,167[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:20:14,194[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:20:14,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:20:14,195[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:20:14,199[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:20:16,818[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H5_ELU           â”‚ 25.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 25.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ELU              â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ELU              â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ELU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ELU              â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ ELU              â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚     66 â”‚
â”‚ 13 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 14 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 16 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 17 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        299.47it/s loss: 0.0129     
                                   0:00:00                     v_num: 1qao      
                                                               val/acc: 0.02    
                                                               val/acc_best:    
                                                               0.017 train/acc: 
                                                               0.019            
[[36m2022-06-16 13:24:14,184[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:24:14,695[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ELU_128_64_32/2022-06-16_13-20-08/experiment=NN/H5_ELU.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:24:14,698[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:24:14,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ELU_128_64_32/2022-06-16_13-20-08/experiment=NN/H5_ELU.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    0.07156991958618164    â”‚
â”‚         test/loss         â”‚    0.07156991958618164    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 813.23it/s 
[[36m2022-06-16 13:24:15,161[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.392 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–„â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–„â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 0.07157
wandb:           test/loss 0.07157
wandb:           train/acc 0.01926
wandb:          train/loss 0.01926
wandb: trainer/global_step 62500
wandb:             val/acc 0.01954
wandb:        val/acc_best 0.01683
wandb:            val/loss 0.01954
wandb: 
wandb: Synced H5_ELU_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/1cl51qao
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_132010-1cl51qao/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fcf281704c0>
True
[[36m2022-06-16 13:24:19,284[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_ELU_128_64_32/2022-06-16_13-20-08/experiment=NN/H5_ELU.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:24:21,269[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:24:21,269[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H3_Softshrink.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:24:22,308[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:24:22,309[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H3_Softshrink_128_64_32                                           
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H3_Softshrink                  
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H3_Softshrink_128_64_32                                         
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H3_Softshrink_128_64_32                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H3_Softshrink_128_64_32                                                 
[[36m2022-06-16 13:24:22,339[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:24:22,342[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:24:22,535[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpzw1hajxn[0m
[[36m2022-06-16 13:24:22,535[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpzw1hajxn/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:24:22,541[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:24:22,543[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:24:22,543[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:24:22,543[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:24:22,544[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_132423-33p0yrbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H3_Softshrink_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/33p0yrbc
[[36m2022-06-16 13:24:27,263[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:24:27,290[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:24:27,291[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:24:27,291[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:24:27,295[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:24:30,037[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H3_Softshrink    â”‚ 23.3 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 23.3 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚     66 â”‚
â”‚ 9  â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 10 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 11 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 12 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 23.3 K                                                        
Non-trainable params: 0                                                         
Total params: 23.3 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        338.98it/s loss: 0.0695     
                                   0:00:00                     v_num: yrbc      
                                                               val/acc: 0.072   
                                                               val/acc_best:    
                                                               0.066 train/acc: 
                                                               0.082            
[[36m2022-06-16 13:27:59,990[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:28:00,610[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_Softshrink_128_64_32/2022-06-16_13-24-21/experiment=NN/H3_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:28:00,613[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:28:00,614[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_Softshrink_128_64_32/2022-06-16_13-24-21/experiment=NN/H3_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    1.8656529188156128     â”‚
â”‚         test/loss         â”‚    1.8656529188156128     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 825.58it/s 
[[36m2022-06-16 13:28:01,077[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.390 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.390 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.394 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: - 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: \ 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: | 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb: / 0.407 MB of 0.407 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 1.86565
wandb:           test/loss 1.86565
wandb:           train/acc 0.08166
wandb:          train/loss 0.08166
wandb: trainer/global_step 62500
wandb:             val/acc 0.07181
wandb:        val/acc_best 0.06642
wandb:            val/loss 0.07181
wandb: 
wandb: Synced H3_Softshrink_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/33p0yrbc
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_132423-33p0yrbc/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fd3c9b43fa0>
True
[[36m2022-06-16 13:28:05,651[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H3_Softshrink_128_64_32/2022-06-16_13-24-21/experiment=NN/H3_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:28:07,637[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:28:07,637[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H4_Softshrink.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:28:08,703[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:28:08,703[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H4_Softshrink_128_64_32                                           
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H4_Softshrink                  
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H4_Softshrink_128_64_32                                         
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H4_Softshrink_128_64_32                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H4_Softshrink_128_64_32                                                 
[[36m2022-06-16 13:28:08,733[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:28:08,736[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:28:08,926[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpv3ixsbft[0m
[[36m2022-06-16 13:28:08,926[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpv3ixsbft/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:28:08,932[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:28:08,933[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:28:08,934[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:28:08,934[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:28:08,935[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_132809-iivc1rm6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H4_Softshrink_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/iivc1rm6
[[36m2022-06-16 13:28:13,622[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:28:13,650[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:28:13,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:28:13,651[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:28:13,655[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:28:16,305[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H4_Softshrink    â”‚ 24.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 24.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚     66 â”‚
â”‚ 11 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 12 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 13 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 14 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 24.4 K                                                        
Non-trainable params: 0                                                         
Total params: 24.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        311.60it/s loss: 21.3 v_num:
                                   0:00:00                     1rm6 val/acc:    
                                                               20.719           
                                                               val/acc_best:    
                                                               20.717 train/acc:
                                                               20.887           
[[36m2022-06-16 13:32:00,797[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:32:01,386[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_Softshrink_128_64_32/2022-06-16_13-28-07/experiment=NN/H4_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:32:01,389[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:32:01,390[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_Softshrink_128_64_32/2022-06-16_13-28-07/experiment=NN/H4_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    31.097734451293945     â”‚
â”‚         test/loss         â”‚    31.097734451293945     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 799.14it/s 
[[36m2022-06-16 13:32:01,850[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: | 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.409 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.409 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 31.09773
wandb:           test/loss 31.09773
wandb:           train/acc 20.88711
wandb:          train/loss 20.88711
wandb: trainer/global_step 62500
wandb:             val/acc 20.71862
wandb:        val/acc_best 20.71737
wandb:            val/loss 20.71862
wandb: 
wandb: Synced H4_Softshrink_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/iivc1rm6
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_132809-iivc1rm6/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f41a5e9beb0>
True
[[36m2022-06-16 13:32:05,889[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H4_Softshrink_128_64_32/2022-06-16_13-28-07/experiment=NN/H4_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
/home/ryuichi/machine_learning/nngauss/train.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="configs/", config_name="train.yaml")
[[36m2022-06-16 13:32:07,875[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-06-16 13:32:07,875[0m][[35mHYDRA[0m] 	#0 : experiment=NN/H5_Softshrink.yaml[0m
/home/ryuichi/mambaforge/envs/lhtrain/lib/python3.9/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[[36m2022-06-16 13:32:08,919[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-06-16 13:32:08,919[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.gauss_datamodule.gaussDataModule              
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 40000                                                                 
â”‚       - 5000                                                                  
â”‚       - 5000                                                                  
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.gauss_module.gaussLitModule                        
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0005                                                    
â”‚       data_dir: /home/ryuichi/machine_learning/nngauss/data/                  
â”‚       name: H5_Softshrink_128_64_32                                           
â”‚       net:                                                                    
â”‚         _target_: src.models.components.NN_net.H5_Softshrink                  
â”‚         input_size: 100                                                       
â”‚         lin1_size: 128                                                        
â”‚         lin2_size: 64                                                         
â”‚         lin3_size: 32                                                         
â”‚         output_size: 2                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         mode: max                                                             
â”‚         patience: 100                                                         
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         project: nngauss_NN                                                   
â”‚         name: H5_Softshrink_128_64_32                                         
â”‚         save_dir: /tmp                                                        
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         job_type: train                                                       
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - gauss                                                               
â”‚         - H5_Softshrink_128_64_32                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       gpus: 1                                                                 
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       resume_from_checkpoint: null                                            
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss                                  
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /home/ryuichi/machine_learning/nngauss/data/                            
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 0                                                                       
â””â”€â”€ name
    â””â”€â”€ H5_Softshrink_128_64_32                                                 
[[36m2022-06-16 13:32:08,949[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.gauss_datamodule.gaussDataModule>[0m
[[36m2022-06-16 13:32:08,952[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.gauss_module.gaussLitModule>[0m
[[36m2022-06-16 13:32:09,138[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp9imu8t4b[0m
[[36m2022-06-16 13:32:09,139[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp9imu8t4b/_remote_module_non_sriptable.py[0m
[[36m2022-06-16 13:32:09,145[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-06-16 13:32:09,146[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-06-16 13:32:09,147[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-06-16 13:32:09,147[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-06-16 13:32:09,147[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: rshimogawa. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /tmp/wandb/run-20220616_133210-3inzguao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run H5_Softshrink_128_64_32
wandb: â­ï¸ View project at https://wandb.ai/rshimogawa/nngauss_NN
wandb: ğŸš€ View run at https://wandb.ai/rshimogawa/nngauss_NN/runs/3inzguao
[[36m2022-06-16 13:32:13,852[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-06-16 13:32:13,878[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-06-16 13:32:13,879[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-06-16 13:32:13,879[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-06-16 13:32:13,883[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-06-16 13:32:16,531[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type             â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ H5_Softshrink    â”‚ 25.4 K â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential       â”‚ 25.4 K â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear           â”‚ 12.9 K â”‚
â”‚ 3  â”‚ net.model.1  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear           â”‚  8.3 K â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear           â”‚  2.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear           â”‚  1.1 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Softshrink       â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear           â”‚     66 â”‚
â”‚ 13 â”‚ criterion    â”‚ MSELoss          â”‚      0 â”‚
â”‚ 14 â”‚ train_acc    â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 15 â”‚ val_acc      â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 16 â”‚ test_acc     â”‚ MeanSquaredError â”‚      0 â”‚
â”‚ 17 â”‚ val_acc_best â”‚ MinMetric        â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 25.4 K                                                        
Non-trainable params: 0                                                         
Total params: 25.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 99  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 704/704 0:00:02 â€¢        299.48it/s loss: 20.3 v_num:
                                   0:00:00                     guao val/acc:    
                                                               20.719           
                                                               val/acc_best:    
                                                               20.717 train/acc:
                                                               20.887           
[[36m2022-06-16 13:36:14,297[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-06-16 13:36:14,906[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_Softshrink_128_64_32/2022-06-16_13-32-07/experiment=NN/H5_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
[[36m2022-06-16 13:36:14,910[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1][0m
[[36m2022-06-16 13:36:14,910[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_Softshrink_128_64_32/2022-06-16_13-32-07/experiment=NN/H5_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/acc          â”‚    29.695537567138672     â”‚
â”‚         test/loss         â”‚    29.695537567138672     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 79/79 0:00:00 â€¢ 0:00:00 825.27it/s 
[[36m2022-06-16 13:36:15,376[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.404 MB uploaded (0.000 MB deduped)wandb: | 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.404 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: | 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: / 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: - 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb: \ 0.421 MB of 0.421 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:           test/loss â–
wandb:           train/acc â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val/acc â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        val/acc_best â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val/loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:            test/acc 29.69554
wandb:           test/loss 29.69554
wandb:           train/acc 20.88716
wandb:          train/loss 20.88716
wandb: trainer/global_step 62500
wandb:             val/acc 20.71916
wandb:        val/acc_best 20.71712
wandb:            val/loss 20.71916
wandb: 
wandb: Synced H5_Softshrink_128_64_32: https://wandb.ai/rshimogawa/nngauss_NN/runs/3inzguao
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20220616_133210-3inzguao/logs
<pytorch_lightning.loggers.wandb.WandbLogger object at 0x7fec8fb193d0>
True
[[36m2022-06-16 13:36:20,203[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /home/ryuichi/machine_learning/nngauss/logs/experiments/multiruns/H5_Softshrink_128_64_32/2022-06-16_13-32-07/experiment=NN/H5_Softshrink.yaml/checkpoints/epoch_000.ckpt[0m
